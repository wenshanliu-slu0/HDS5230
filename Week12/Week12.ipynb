{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HDS5230- Week12 Assignmnent\n",
    "\n",
    "Author: Wen-shan, Liu\n",
    "\n",
    "Date: 05-03-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 9)\n",
      "   pregnant  glucose  pressure  triceps  insulin  mass  pedigree  age  outcome\n",
      "0         0      100        86       39       49  30.0     0.966   22        0\n",
      "1         0      128        70       31      510  30.0     0.543   31        0\n",
      "2         0       81        70       41      271  22.1     0.826   24        0\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "import pandas as pd\n",
    "# df1 = dataset 1000\n",
    "df1 = pd.read_csv('dataset/1000_.csv')\n",
    "\n",
    "# check the shape and head of the dataset\n",
    "print(df1.shape)\n",
    "print(df1.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9)\n",
      "   pregnant  glucose  pressure  triceps  insulin  mass  pedigree  age  outcome\n",
      "0         1      127        72       16      142  27.6     0.497   33        0\n",
      "1         2      119        50       23      140  36.6     0.430   25        0\n",
      "2         1      154        88       46       90  25.4     0.439   28        0\n"
     ]
    }
   ],
   "source": [
    "# df2 = dataset 10000\n",
    "df2 = pd.read_csv('dataset/10000_.csv')\n",
    "print(df2.shape)\n",
    "print(df2.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 9)\n",
      "   pregnant  glucose  pressure  triceps  insulin  mass  pedigree  age  outcome\n",
      "0         3       90        66       11      140  33.2     0.176   25        0\n",
      "1        10      188        62       44      115  35.9     0.408   25        1\n",
      "2         3       83        60       29       25  35.5     2.420   26        0\n"
     ]
    }
   ],
   "source": [
    "# df3 = dataset 100000\n",
    "df3 = pd.read_csv('dataset/100000_.csv')\n",
    "print(df3.shape)\n",
    "print(df3.head(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# train_MLP function : 1 hidden layer, 4 nodes, iteration 500, activation function is relu\n",
    "def  train_MLP(df):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('outcome', axis=1), df['outcome'], test_size=0.2, random_state=42)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 1 hidden layer, 4 nodes, iteration 500, activation function is relu\n",
    "    model = MLPClassifier(hidden_layer_sizes=(4,), activation='relu', max_iter=500, \n",
    "                        verbose=True, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # evaluate the model\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    val_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "    train_error = 1- train_acc\n",
    "    val_error = 1- val_acc\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "    print(f'df.shape: {df.shape}')\n",
    "    print(f'X_train.shape: {X_train.shape}')\n",
    "    print(f'X_test.shape: {X_test.shape}')\n",
    "    print(f'y_train.shape: {y_train.shape}')\n",
    "    print(f'y_test.shape: {y_test.shape}')\n",
    "    print(f\"Training Time: {total_time:.4f} seconds\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"train_error: {train_error:.4f}\")\n",
    "    print(f\"val_error: {val_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.02291327\n",
      "Iteration 2, loss = 1.91983074\n",
      "Iteration 3, loss = 1.82442250\n",
      "Iteration 4, loss = 1.73193378\n",
      "Iteration 5, loss = 1.64181016\n",
      "Iteration 6, loss = 1.56161228\n",
      "Iteration 7, loss = 1.48380261\n",
      "Iteration 8, loss = 1.41037761\n",
      "Iteration 9, loss = 1.34081399\n",
      "Iteration 10, loss = 1.28076026\n",
      "Iteration 11, loss = 1.22147491\n",
      "Iteration 12, loss = 1.16632754\n",
      "Iteration 13, loss = 1.11748362\n",
      "Iteration 14, loss = 1.07141995\n",
      "Iteration 15, loss = 1.02971748\n",
      "Iteration 16, loss = 0.98947785\n",
      "Iteration 17, loss = 0.95173200\n",
      "Iteration 18, loss = 0.92012273\n",
      "Iteration 19, loss = 0.88814560\n",
      "Iteration 20, loss = 0.85932503\n",
      "Iteration 21, loss = 0.83110290\n",
      "Iteration 22, loss = 0.80776051\n",
      "Iteration 23, loss = 0.78431582\n",
      "Iteration 24, loss = 0.76472395\n",
      "Iteration 25, loss = 0.74543579\n",
      "Iteration 26, loss = 0.72986348\n",
      "Iteration 27, loss = 0.71409713\n",
      "Iteration 28, loss = 0.70001601\n",
      "Iteration 29, loss = 0.68726781\n",
      "Iteration 30, loss = 0.67486463\n",
      "Iteration 31, loss = 0.66428553\n",
      "Iteration 32, loss = 0.65455384\n",
      "Iteration 33, loss = 0.64605412\n",
      "Iteration 34, loss = 0.63760811\n",
      "Iteration 35, loss = 0.63013460\n",
      "Iteration 36, loss = 0.62284607\n",
      "Iteration 37, loss = 0.61708185\n",
      "Iteration 38, loss = 0.61100924\n",
      "Iteration 39, loss = 0.60602039\n",
      "Iteration 40, loss = 0.60109457\n",
      "Iteration 41, loss = 0.59676266\n",
      "Iteration 42, loss = 0.59266668\n",
      "Iteration 43, loss = 0.58940697\n",
      "Iteration 44, loss = 0.58582503\n",
      "Iteration 45, loss = 0.58255376\n",
      "Iteration 46, loss = 0.58011222\n",
      "Iteration 47, loss = 0.57747150\n",
      "Iteration 48, loss = 0.57528574\n",
      "Iteration 49, loss = 0.57325492\n",
      "Iteration 50, loss = 0.57123612\n",
      "Iteration 51, loss = 0.56953032\n",
      "Iteration 52, loss = 0.56782482\n",
      "Iteration 53, loss = 0.56646094\n",
      "Iteration 54, loss = 0.56510699\n",
      "Iteration 55, loss = 0.56400197\n",
      "Iteration 56, loss = 0.56255395\n",
      "Iteration 57, loss = 0.56168057\n",
      "Iteration 58, loss = 0.56059706\n",
      "Iteration 59, loss = 0.55971078\n",
      "Iteration 60, loss = 0.55893913\n",
      "Iteration 61, loss = 0.55800769\n",
      "Iteration 62, loss = 0.55727463\n",
      "Iteration 63, loss = 0.55655074\n",
      "Iteration 64, loss = 0.55604384\n",
      "Iteration 65, loss = 0.55538381\n",
      "Iteration 66, loss = 0.55476012\n",
      "Iteration 67, loss = 0.55414965\n",
      "Iteration 68, loss = 0.55358373\n",
      "Iteration 69, loss = 0.55309990\n",
      "Iteration 70, loss = 0.55259476\n",
      "Iteration 71, loss = 0.55229205\n",
      "Iteration 72, loss = 0.55169723\n",
      "Iteration 73, loss = 0.55133415\n",
      "Iteration 74, loss = 0.55071256\n",
      "Iteration 75, loss = 0.55036625\n",
      "Iteration 76, loss = 0.54987679\n",
      "Iteration 77, loss = 0.54938426\n",
      "Iteration 78, loss = 0.54891782\n",
      "Iteration 79, loss = 0.54851567\n",
      "Iteration 80, loss = 0.54821132\n",
      "Iteration 81, loss = 0.54772802\n",
      "Iteration 82, loss = 0.54741454\n",
      "Iteration 83, loss = 0.54699226\n",
      "Iteration 84, loss = 0.54658439\n",
      "Iteration 85, loss = 0.54624124\n",
      "Iteration 86, loss = 0.54592874\n",
      "Iteration 87, loss = 0.54554296\n",
      "Iteration 88, loss = 0.54535959\n",
      "Iteration 89, loss = 0.54481993\n",
      "Iteration 90, loss = 0.54452812\n",
      "Iteration 91, loss = 0.54416076\n",
      "Iteration 92, loss = 0.54422079\n",
      "Iteration 93, loss = 0.54393476\n",
      "Iteration 94, loss = 0.54333905\n",
      "Iteration 95, loss = 0.54302255\n",
      "Iteration 96, loss = 0.54285407\n",
      "Iteration 97, loss = 0.54242211\n",
      "Iteration 98, loss = 0.54220355\n",
      "Iteration 99, loss = 0.54201903\n",
      "Iteration 100, loss = 0.54159443\n",
      "Iteration 101, loss = 0.54115376\n",
      "Iteration 102, loss = 0.54095359\n",
      "Iteration 103, loss = 0.54056416\n",
      "Iteration 104, loss = 0.54030556\n",
      "Iteration 105, loss = 0.54004996\n",
      "Iteration 106, loss = 0.53976267\n",
      "Iteration 107, loss = 0.53984658\n",
      "Iteration 108, loss = 0.53931784\n",
      "Iteration 109, loss = 0.53900720\n",
      "Iteration 110, loss = 0.53873942\n",
      "Iteration 111, loss = 0.53855906\n",
      "Iteration 112, loss = 0.53823247\n",
      "Iteration 113, loss = 0.53809657\n",
      "Iteration 114, loss = 0.53783217\n",
      "Iteration 115, loss = 0.53729520\n",
      "Iteration 116, loss = 0.53750555\n",
      "Iteration 117, loss = 0.53708740\n",
      "Iteration 118, loss = 0.53693540\n",
      "Iteration 119, loss = 0.53667324\n",
      "Iteration 120, loss = 0.53646976\n",
      "Iteration 121, loss = 0.53608201\n",
      "Iteration 122, loss = 0.53571254\n",
      "Iteration 123, loss = 0.53553201\n",
      "Iteration 124, loss = 0.53533043\n",
      "Iteration 125, loss = 0.53510609\n",
      "Iteration 126, loss = 0.53477362\n",
      "Iteration 127, loss = 0.53456630\n",
      "Iteration 128, loss = 0.53431187\n",
      "Iteration 129, loss = 0.53406057\n",
      "Iteration 130, loss = 0.53383235\n",
      "Iteration 131, loss = 0.53378608\n",
      "Iteration 132, loss = 0.53343906\n",
      "Iteration 133, loss = 0.53310231\n",
      "Iteration 134, loss = 0.53266418\n",
      "Iteration 135, loss = 0.53262252\n",
      "Iteration 136, loss = 0.53236859\n",
      "Iteration 137, loss = 0.53222896\n",
      "Iteration 138, loss = 0.53190356\n",
      "Iteration 139, loss = 0.53143203\n",
      "Iteration 140, loss = 0.53101689\n",
      "Iteration 141, loss = 0.53074330\n",
      "Iteration 142, loss = 0.53044465\n",
      "Iteration 143, loss = 0.53005929\n",
      "Iteration 144, loss = 0.52979130\n",
      "Iteration 145, loss = 0.52953147\n",
      "Iteration 146, loss = 0.52915709\n",
      "Iteration 147, loss = 0.52902269\n",
      "Iteration 148, loss = 0.52866282\n",
      "Iteration 149, loss = 0.52882984\n",
      "Iteration 150, loss = 0.52821694\n",
      "Iteration 151, loss = 0.52777526\n",
      "Iteration 152, loss = 0.52751239\n",
      "Iteration 153, loss = 0.52712000\n",
      "Iteration 154, loss = 0.52689457\n",
      "Iteration 155, loss = 0.52655480\n",
      "Iteration 156, loss = 0.52640615\n",
      "Iteration 157, loss = 0.52626739\n",
      "Iteration 158, loss = 0.52588516\n",
      "Iteration 159, loss = 0.52547043\n",
      "Iteration 160, loss = 0.52531756\n",
      "Iteration 161, loss = 0.52521447\n",
      "Iteration 162, loss = 0.52496934\n",
      "Iteration 163, loss = 0.52441381\n",
      "Iteration 164, loss = 0.52427560\n",
      "Iteration 165, loss = 0.52405055\n",
      "Iteration 166, loss = 0.52378990\n",
      "Iteration 167, loss = 0.52355088\n",
      "Iteration 168, loss = 0.52329961\n",
      "Iteration 169, loss = 0.52310120\n",
      "Iteration 170, loss = 0.52292702\n",
      "Iteration 171, loss = 0.52273868\n",
      "Iteration 172, loss = 0.52223301\n",
      "Iteration 173, loss = 0.52187330\n",
      "Iteration 174, loss = 0.52155642\n",
      "Iteration 175, loss = 0.52127521\n",
      "Iteration 176, loss = 0.52095965\n",
      "Iteration 177, loss = 0.52088837\n",
      "Iteration 178, loss = 0.52038335\n",
      "Iteration 179, loss = 0.52004891\n",
      "Iteration 180, loss = 0.51965446\n",
      "Iteration 181, loss = 0.51933876\n",
      "Iteration 182, loss = 0.51881951\n",
      "Iteration 183, loss = 0.51865668\n",
      "Iteration 184, loss = 0.51822136\n",
      "Iteration 185, loss = 0.51780112\n",
      "Iteration 186, loss = 0.51749771\n",
      "Iteration 187, loss = 0.51682423\n",
      "Iteration 188, loss = 0.51682926\n",
      "Iteration 189, loss = 0.51628922\n",
      "Iteration 190, loss = 0.51578284\n",
      "Iteration 191, loss = 0.51582896\n",
      "Iteration 192, loss = 0.51581601\n",
      "Iteration 193, loss = 0.51527491\n",
      "Iteration 194, loss = 0.51506207\n",
      "Iteration 195, loss = 0.51484517\n",
      "Iteration 196, loss = 0.51429052\n",
      "Iteration 197, loss = 0.51400911\n",
      "Iteration 198, loss = 0.51378521\n",
      "Iteration 199, loss = 0.51333807\n",
      "Iteration 200, loss = 0.51322329\n",
      "Iteration 201, loss = 0.51278462\n",
      "Iteration 202, loss = 0.51224614\n",
      "Iteration 203, loss = 0.51167942\n",
      "Iteration 204, loss = 0.51142003\n",
      "Iteration 205, loss = 0.51086231\n",
      "Iteration 206, loss = 0.51048516\n",
      "Iteration 207, loss = 0.51006684\n",
      "Iteration 208, loss = 0.50977359\n",
      "Iteration 209, loss = 0.50923094\n",
      "Iteration 210, loss = 0.50888314\n",
      "Iteration 211, loss = 0.50851075\n",
      "Iteration 212, loss = 0.50818582\n",
      "Iteration 213, loss = 0.50769884\n",
      "Iteration 214, loss = 0.50736365\n",
      "Iteration 215, loss = 0.50694968\n",
      "Iteration 216, loss = 0.50673989\n",
      "Iteration 217, loss = 0.50634826\n",
      "Iteration 218, loss = 0.50590057\n",
      "Iteration 219, loss = 0.50557169\n",
      "Iteration 220, loss = 0.50520654\n",
      "Iteration 221, loss = 0.50484081\n",
      "Iteration 222, loss = 0.50461863\n",
      "Iteration 223, loss = 0.50432546\n",
      "Iteration 224, loss = 0.50393799\n",
      "Iteration 225, loss = 0.50353426\n",
      "Iteration 226, loss = 0.50317323\n",
      "Iteration 227, loss = 0.50278852\n",
      "Iteration 228, loss = 0.50243374\n",
      "Iteration 229, loss = 0.50198787\n",
      "Iteration 230, loss = 0.50203514\n",
      "Iteration 231, loss = 0.50150679\n",
      "Iteration 232, loss = 0.50183422\n",
      "Iteration 233, loss = 0.50082962\n",
      "Iteration 234, loss = 0.50113918\n",
      "Iteration 235, loss = 0.50074197\n",
      "Iteration 236, loss = 0.50021233\n",
      "Iteration 237, loss = 0.50026245\n",
      "Iteration 238, loss = 0.49989309\n",
      "Iteration 239, loss = 0.49976436\n",
      "Iteration 240, loss = 0.49960438\n",
      "Iteration 241, loss = 0.49933467\n",
      "Iteration 242, loss = 0.49918742\n",
      "Iteration 243, loss = 0.49899431\n",
      "Iteration 244, loss = 0.49883914\n",
      "Iteration 245, loss = 0.49862427\n",
      "Iteration 246, loss = 0.49847163\n",
      "Iteration 247, loss = 0.49815020\n",
      "Iteration 248, loss = 0.49813297\n",
      "Iteration 249, loss = 0.49790064\n",
      "Iteration 250, loss = 0.49761621\n",
      "Iteration 251, loss = 0.49738659\n",
      "Iteration 252, loss = 0.49737194\n",
      "Iteration 253, loss = 0.49726437\n",
      "Iteration 254, loss = 0.49690385\n",
      "Iteration 255, loss = 0.49675831\n",
      "Iteration 256, loss = 0.49666154\n",
      "Iteration 257, loss = 0.49644712\n",
      "Iteration 258, loss = 0.49615268\n",
      "Iteration 259, loss = 0.49595704\n",
      "Iteration 260, loss = 0.49594704\n",
      "Iteration 261, loss = 0.49571237\n",
      "Iteration 262, loss = 0.49582412\n",
      "Iteration 263, loss = 0.49565877\n",
      "Iteration 264, loss = 0.49541346\n",
      "Iteration 265, loss = 0.49507533\n",
      "Iteration 266, loss = 0.49508272\n",
      "Iteration 267, loss = 0.49510198\n",
      "Iteration 268, loss = 0.49472413\n",
      "Iteration 269, loss = 0.49465128\n",
      "Iteration 270, loss = 0.49426781\n",
      "Iteration 271, loss = 0.49398566\n",
      "Iteration 272, loss = 0.49386519\n",
      "Iteration 273, loss = 0.49370438\n",
      "Iteration 274, loss = 0.49361099\n",
      "Iteration 275, loss = 0.49343059\n",
      "Iteration 276, loss = 0.49333312\n",
      "Iteration 277, loss = 0.49311792\n",
      "Iteration 278, loss = 0.49320656\n",
      "Iteration 279, loss = 0.49295486\n",
      "Iteration 280, loss = 0.49273047\n",
      "Iteration 281, loss = 0.49262279\n",
      "Iteration 282, loss = 0.49242607\n",
      "Iteration 283, loss = 0.49246201\n",
      "Iteration 284, loss = 0.49219474\n",
      "Iteration 285, loss = 0.49217227\n",
      "Iteration 286, loss = 0.49193580\n",
      "Iteration 287, loss = 0.49191176\n",
      "Iteration 288, loss = 0.49165418\n",
      "Iteration 289, loss = 0.49157832\n",
      "Iteration 290, loss = 0.49137936\n",
      "Iteration 291, loss = 0.49133527\n",
      "Iteration 292, loss = 0.49107745\n",
      "Iteration 293, loss = 0.49092258\n",
      "Iteration 294, loss = 0.49083739\n",
      "Iteration 295, loss = 0.49066464\n",
      "Iteration 296, loss = 0.49053253\n",
      "Iteration 297, loss = 0.49035977\n",
      "Iteration 298, loss = 0.49040448\n",
      "Iteration 299, loss = 0.49017320\n",
      "Iteration 300, loss = 0.49018312\n",
      "Iteration 301, loss = 0.48994846\n",
      "Iteration 302, loss = 0.48975443\n",
      "Iteration 303, loss = 0.48968790\n",
      "Iteration 304, loss = 0.48948562\n",
      "Iteration 305, loss = 0.48945138\n",
      "Iteration 306, loss = 0.48924022\n",
      "Iteration 307, loss = 0.48918594\n",
      "Iteration 308, loss = 0.48896681\n",
      "Iteration 309, loss = 0.48876277\n",
      "Iteration 310, loss = 0.48884760\n",
      "Iteration 311, loss = 0.48866464\n",
      "Iteration 312, loss = 0.48862449\n",
      "Iteration 313, loss = 0.48846187\n",
      "Iteration 314, loss = 0.48826056\n",
      "Iteration 315, loss = 0.48809933\n",
      "Iteration 316, loss = 0.48790789\n",
      "Iteration 317, loss = 0.48780939\n",
      "Iteration 318, loss = 0.48761986\n",
      "Iteration 319, loss = 0.48752850\n",
      "Iteration 320, loss = 0.48737091\n",
      "Iteration 321, loss = 0.48739329\n",
      "Iteration 322, loss = 0.48715905\n",
      "Iteration 323, loss = 0.48707155\n",
      "Iteration 324, loss = 0.48690418\n",
      "Iteration 325, loss = 0.48670758\n",
      "Iteration 326, loss = 0.48664708\n",
      "Iteration 327, loss = 0.48653906\n",
      "Iteration 328, loss = 0.48625038\n",
      "Iteration 329, loss = 0.48617017\n",
      "Iteration 330, loss = 0.48616568\n",
      "Iteration 331, loss = 0.48587582\n",
      "Iteration 332, loss = 0.48578947\n",
      "Iteration 333, loss = 0.48571308\n",
      "Iteration 334, loss = 0.48552794\n",
      "Iteration 335, loss = 0.48530121\n",
      "Iteration 336, loss = 0.48519848\n",
      "Iteration 337, loss = 0.48423059\n",
      "Iteration 338, loss = 0.48023180\n",
      "Iteration 339, loss = 0.47967093\n",
      "Iteration 340, loss = 0.46624772\n",
      "Iteration 341, loss = 0.46627230\n",
      "Iteration 342, loss = 0.45788661\n",
      "Iteration 343, loss = 0.45124165\n",
      "Iteration 344, loss = 0.44136909\n",
      "Iteration 345, loss = 0.43788013\n",
      "Iteration 346, loss = 0.43182801\n",
      "Iteration 347, loss = 0.42507324\n",
      "Iteration 348, loss = 0.42084785\n",
      "Iteration 349, loss = 0.41685607\n",
      "Iteration 350, loss = 0.41530777\n",
      "Iteration 351, loss = 0.40726781\n",
      "Iteration 352, loss = 0.40741741\n",
      "Iteration 353, loss = 0.40321102\n",
      "Iteration 354, loss = 0.39910327\n",
      "Iteration 355, loss = 0.40006151\n",
      "Iteration 356, loss = 0.39636657\n",
      "Iteration 357, loss = 0.39355134\n",
      "Iteration 358, loss = 0.39164458\n",
      "Iteration 359, loss = 0.38881183\n",
      "Iteration 360, loss = 0.38880323\n",
      "Iteration 361, loss = 0.38478224\n",
      "Iteration 362, loss = 0.38442967\n",
      "Iteration 363, loss = 0.38491646\n",
      "Iteration 364, loss = 0.38280307\n",
      "Iteration 365, loss = 0.38112483\n",
      "Iteration 366, loss = 0.38334772\n",
      "Iteration 367, loss = 0.37651093\n",
      "Iteration 368, loss = 0.37866055\n",
      "Iteration 369, loss = 0.37577326\n",
      "Iteration 370, loss = 0.37438424\n",
      "Iteration 371, loss = 0.37436598\n",
      "Iteration 372, loss = 0.37166020\n",
      "Iteration 373, loss = 0.37255149\n",
      "Iteration 374, loss = 0.37227334\n",
      "Iteration 375, loss = 0.37438214\n",
      "Iteration 376, loss = 0.36840469\n",
      "Iteration 377, loss = 0.37110674\n",
      "Iteration 378, loss = 0.36653840\n",
      "Iteration 379, loss = 0.36706707\n",
      "Iteration 380, loss = 0.36548903\n",
      "Iteration 381, loss = 0.36469783\n",
      "Iteration 382, loss = 0.36827052\n",
      "Iteration 383, loss = 0.36296624\n",
      "Iteration 384, loss = 0.36389433\n",
      "Iteration 385, loss = 0.36148944\n",
      "Iteration 386, loss = 0.36154801\n",
      "Iteration 387, loss = 0.35976199\n",
      "Iteration 388, loss = 0.36117568\n",
      "Iteration 389, loss = 0.35999612\n",
      "Iteration 390, loss = 0.36028816\n",
      "Iteration 391, loss = 0.36279906\n",
      "Iteration 392, loss = 0.35875640\n",
      "Iteration 393, loss = 0.36008582\n",
      "Iteration 394, loss = 0.35826128\n",
      "Iteration 395, loss = 0.35696129\n",
      "Iteration 396, loss = 0.35836564\n",
      "Iteration 397, loss = 0.35630885\n",
      "Iteration 398, loss = 0.35564373\n",
      "Iteration 399, loss = 0.35354857\n",
      "Iteration 400, loss = 0.35380902\n",
      "Iteration 401, loss = 0.35328551\n",
      "Iteration 402, loss = 0.35451106\n",
      "Iteration 403, loss = 0.35142438\n",
      "Iteration 404, loss = 0.35210544\n",
      "Iteration 405, loss = 0.35164871\n",
      "Iteration 406, loss = 0.35293617\n",
      "Iteration 407, loss = 0.35621875\n",
      "Iteration 408, loss = 0.35131635\n",
      "Iteration 409, loss = 0.35554439\n",
      "Iteration 410, loss = 0.34960251\n",
      "Iteration 411, loss = 0.35231858\n",
      "Iteration 412, loss = 0.35023105\n",
      "Iteration 413, loss = 0.35042189\n",
      "Iteration 414, loss = 0.34932157\n",
      "Iteration 415, loss = 0.34840103\n",
      "Iteration 416, loss = 0.34828176\n",
      "Iteration 417, loss = 0.34892934\n",
      "Iteration 418, loss = 0.34686177\n",
      "Iteration 419, loss = 0.34668861\n",
      "Iteration 420, loss = 0.34726559\n",
      "Iteration 421, loss = 0.34771877\n",
      "Iteration 422, loss = 0.34561370\n",
      "Iteration 423, loss = 0.34607976\n",
      "Iteration 424, loss = 0.34497363\n",
      "Iteration 425, loss = 0.34550534\n",
      "Iteration 426, loss = 0.34507736\n",
      "Iteration 427, loss = 0.34608704\n",
      "Iteration 428, loss = 0.34469388\n",
      "Iteration 429, loss = 0.34532309\n",
      "Iteration 430, loss = 0.34408602\n",
      "Iteration 431, loss = 0.34466404\n",
      "Iteration 432, loss = 0.34425459\n",
      "Iteration 433, loss = 0.34379983\n",
      "Iteration 434, loss = 0.34444236\n",
      "Iteration 435, loss = 0.34336225\n",
      "Iteration 436, loss = 0.34350496\n",
      "Iteration 437, loss = 0.34308134\n",
      "Iteration 438, loss = 0.34353495\n",
      "Iteration 439, loss = 0.34276852\n",
      "Iteration 440, loss = 0.34239015\n",
      "Iteration 441, loss = 0.34257484\n",
      "Iteration 442, loss = 0.34278633\n",
      "Iteration 443, loss = 0.34355905\n",
      "Iteration 444, loss = 0.34253374\n",
      "Iteration 445, loss = 0.34228399\n",
      "Iteration 446, loss = 0.34318439\n",
      "Iteration 447, loss = 0.34169855\n",
      "Iteration 448, loss = 0.34166580\n",
      "Iteration 449, loss = 0.34138402\n",
      "Iteration 450, loss = 0.34105149\n",
      "Iteration 451, loss = 0.34062262\n",
      "Iteration 452, loss = 0.34042530\n",
      "Iteration 453, loss = 0.34064053\n",
      "Iteration 454, loss = 0.34013983\n",
      "Iteration 455, loss = 0.34047787\n",
      "Iteration 456, loss = 0.34064717\n",
      "Iteration 457, loss = 0.34054202\n",
      "Iteration 458, loss = 0.33949068\n",
      "Iteration 459, loss = 0.34130844\n",
      "Iteration 460, loss = 0.34014858\n",
      "Iteration 461, loss = 0.33989780\n",
      "Iteration 462, loss = 0.34037250\n",
      "Iteration 463, loss = 0.34044496\n",
      "Iteration 464, loss = 0.33930210\n",
      "Iteration 465, loss = 0.33893914\n",
      "Iteration 466, loss = 0.33886544\n",
      "Iteration 467, loss = 0.33986564\n",
      "Iteration 468, loss = 0.34103733\n",
      "Iteration 469, loss = 0.34007628\n",
      "Iteration 470, loss = 0.33907592\n",
      "Iteration 471, loss = 0.33793809\n",
      "Iteration 472, loss = 0.33806736\n",
      "Iteration 473, loss = 0.33791007\n",
      "Iteration 474, loss = 0.33755784\n",
      "Iteration 475, loss = 0.33789093\n",
      "Iteration 476, loss = 0.33785559\n",
      "Iteration 477, loss = 0.33791511\n",
      "Iteration 478, loss = 0.33750815\n",
      "Iteration 479, loss = 0.33847227\n",
      "Iteration 480, loss = 0.33725538\n",
      "Iteration 481, loss = 0.34048926\n",
      "Iteration 482, loss = 0.33916305\n",
      "Iteration 483, loss = 0.33961153\n",
      "Iteration 484, loss = 0.33991177\n",
      "Iteration 485, loss = 0.33673815\n",
      "Iteration 486, loss = 0.33875907\n",
      "Iteration 487, loss = 0.33750428\n",
      "Iteration 488, loss = 0.33810406\n",
      "Iteration 489, loss = 0.33428837\n",
      "Iteration 490, loss = 0.33653637\n",
      "Iteration 491, loss = 0.33758550\n",
      "Iteration 492, loss = 0.33700076\n",
      "Iteration 493, loss = 0.33619511\n",
      "Iteration 494, loss = 0.33472412\n",
      "Iteration 495, loss = 0.33517987\n",
      "Iteration 496, loss = 0.33539671\n",
      "Iteration 497, loss = 0.33457384\n",
      "Iteration 498, loss = 0.33610628\n",
      "Iteration 499, loss = 0.33488114\n",
      "Iteration 500, loss = 0.33372449\n",
      "--------------------------------\n",
      "df.shape: (1000, 9)\n",
      "X_train.shape: (800, 8)\n",
      "X_test.shape: (200, 8)\n",
      "y_train.shape: (800,)\n",
      "y_test.shape: (200,)\n",
      "Training Time: 0.1963 seconds\n",
      "Training Accuracy: 0.8625\n",
      "Validation Accuracy: 0.8550\n",
      "train_error: 0.1375\n",
      "val_error: 0.1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# execute the train_MLP function by using df1\n",
    "train_MLP(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.57940308\n",
      "Iteration 2, loss = 1.00204516\n",
      "Iteration 3, loss = 0.74438769\n",
      "Iteration 4, loss = 0.63433294\n",
      "Iteration 5, loss = 0.58822106\n",
      "Iteration 6, loss = 0.56845733\n",
      "Iteration 7, loss = 0.56014439\n",
      "Iteration 8, loss = 0.55539844\n",
      "Iteration 9, loss = 0.55164628\n",
      "Iteration 10, loss = 0.54835633\n",
      "Iteration 11, loss = 0.54544182\n",
      "Iteration 12, loss = 0.54275708\n",
      "Iteration 13, loss = 0.54030816\n",
      "Iteration 14, loss = 0.53805771\n",
      "Iteration 15, loss = 0.53595990\n",
      "Iteration 16, loss = 0.53400018\n",
      "Iteration 17, loss = 0.53211780\n",
      "Iteration 18, loss = 0.53043345\n",
      "Iteration 19, loss = 0.52871274\n",
      "Iteration 20, loss = 0.52716174\n",
      "Iteration 21, loss = 0.52549488\n",
      "Iteration 22, loss = 0.52390152\n",
      "Iteration 23, loss = 0.52244874\n",
      "Iteration 24, loss = 0.52070873\n",
      "Iteration 25, loss = 0.51906697\n",
      "Iteration 26, loss = 0.51743123\n",
      "Iteration 27, loss = 0.51585375\n",
      "Iteration 28, loss = 0.51401486\n",
      "Iteration 29, loss = 0.51226048\n",
      "Iteration 30, loss = 0.51056575\n",
      "Iteration 31, loss = 0.50850502\n",
      "Iteration 32, loss = 0.50648829\n",
      "Iteration 33, loss = 0.50431569\n",
      "Iteration 34, loss = 0.50001791\n",
      "Iteration 35, loss = 0.44528628\n",
      "Iteration 36, loss = 0.39468501\n",
      "Iteration 37, loss = 0.37086502\n",
      "Iteration 38, loss = 0.35783469\n",
      "Iteration 39, loss = 0.35157426\n",
      "Iteration 40, loss = 0.34710341\n",
      "Iteration 41, loss = 0.34229583\n",
      "Iteration 42, loss = 0.34309086\n",
      "Iteration 43, loss = 0.33727571\n",
      "Iteration 44, loss = 0.33720479\n",
      "Iteration 45, loss = 0.33493136\n",
      "Iteration 46, loss = 0.33542586\n",
      "Iteration 47, loss = 0.33196740\n",
      "Iteration 48, loss = 0.33637644\n",
      "Iteration 49, loss = 0.33026078\n",
      "Iteration 50, loss = 0.33165645\n",
      "Iteration 51, loss = 0.32993936\n",
      "Iteration 52, loss = 0.32631201\n",
      "Iteration 53, loss = 0.32609670\n",
      "Iteration 54, loss = 0.32448869\n",
      "Iteration 55, loss = 0.32476917\n",
      "Iteration 56, loss = 0.32314994\n",
      "Iteration 57, loss = 0.32308458\n",
      "Iteration 58, loss = 0.32086831\n",
      "Iteration 59, loss = 0.32293880\n",
      "Iteration 60, loss = 0.32280824\n",
      "Iteration 61, loss = 0.32005808\n",
      "Iteration 62, loss = 0.31687443\n",
      "Iteration 63, loss = 0.31728851\n",
      "Iteration 64, loss = 0.31581643\n",
      "Iteration 65, loss = 0.31548124\n",
      "Iteration 66, loss = 0.31650224\n",
      "Iteration 67, loss = 0.31395068\n",
      "Iteration 68, loss = 0.31154018\n",
      "Iteration 69, loss = 0.31162834\n",
      "Iteration 70, loss = 0.31014616\n",
      "Iteration 71, loss = 0.31212604\n",
      "Iteration 72, loss = 0.30852097\n",
      "Iteration 73, loss = 0.30834696\n",
      "Iteration 74, loss = 0.30698339\n",
      "Iteration 75, loss = 0.30578297\n",
      "Iteration 76, loss = 0.30705215\n",
      "Iteration 77, loss = 0.30452841\n",
      "Iteration 78, loss = 0.30343123\n",
      "Iteration 79, loss = 0.30506543\n",
      "Iteration 80, loss = 0.30065464\n",
      "Iteration 81, loss = 0.30045837\n",
      "Iteration 82, loss = 0.30123211\n",
      "Iteration 83, loss = 0.29935923\n",
      "Iteration 84, loss = 0.29651111\n",
      "Iteration 85, loss = 0.29536640\n",
      "Iteration 86, loss = 0.29423040\n",
      "Iteration 87, loss = 0.29612971\n",
      "Iteration 88, loss = 0.29567141\n",
      "Iteration 89, loss = 0.29143507\n",
      "Iteration 90, loss = 0.29123837\n",
      "Iteration 91, loss = 0.29045644\n",
      "Iteration 92, loss = 0.28989338\n",
      "Iteration 93, loss = 0.28833329\n",
      "Iteration 94, loss = 0.29165505\n",
      "Iteration 95, loss = 0.28577180\n",
      "Iteration 96, loss = 0.28715997\n",
      "Iteration 97, loss = 0.28503562\n",
      "Iteration 98, loss = 0.28228761\n",
      "Iteration 99, loss = 0.28177758\n",
      "Iteration 100, loss = 0.28222189\n",
      "Iteration 101, loss = 0.28038578\n",
      "Iteration 102, loss = 0.27900208\n",
      "Iteration 103, loss = 0.27783464\n",
      "Iteration 104, loss = 0.27660134\n",
      "Iteration 105, loss = 0.27597582\n",
      "Iteration 106, loss = 0.27449627\n",
      "Iteration 107, loss = 0.27415440\n",
      "Iteration 108, loss = 0.27183068\n",
      "Iteration 109, loss = 0.27133572\n",
      "Iteration 110, loss = 0.27119502\n",
      "Iteration 111, loss = 0.26863843\n",
      "Iteration 112, loss = 0.26788327\n",
      "Iteration 113, loss = 0.26741471\n",
      "Iteration 114, loss = 0.26474546\n",
      "Iteration 115, loss = 0.26836325\n",
      "Iteration 116, loss = 0.26395906\n",
      "Iteration 117, loss = 0.26623777\n",
      "Iteration 118, loss = 0.26236565\n",
      "Iteration 119, loss = 0.26250944\n",
      "Iteration 120, loss = 0.25907331\n",
      "Iteration 121, loss = 0.26029392\n",
      "Iteration 122, loss = 0.25857563\n",
      "Iteration 123, loss = 0.25538417\n",
      "Iteration 124, loss = 0.25614807\n",
      "Iteration 125, loss = 0.25261907\n",
      "Iteration 126, loss = 0.25183508\n",
      "Iteration 127, loss = 0.25123423\n",
      "Iteration 128, loss = 0.25021885\n",
      "Iteration 129, loss = 0.24811878\n",
      "Iteration 130, loss = 0.24704682\n",
      "Iteration 131, loss = 0.24535662\n",
      "Iteration 132, loss = 0.24426769\n",
      "Iteration 133, loss = 0.24609185\n",
      "Iteration 134, loss = 0.24730865\n",
      "Iteration 135, loss = 0.24040505\n",
      "Iteration 136, loss = 0.24107272\n",
      "Iteration 137, loss = 0.23849384\n",
      "Iteration 138, loss = 0.23677623\n",
      "Iteration 139, loss = 0.23902286\n",
      "Iteration 140, loss = 0.23597948\n",
      "Iteration 141, loss = 0.23594836\n",
      "Iteration 142, loss = 0.23343147\n",
      "Iteration 143, loss = 0.23320626\n",
      "Iteration 144, loss = 0.23847677\n",
      "Iteration 145, loss = 0.23150852\n",
      "Iteration 146, loss = 0.22890711\n",
      "Iteration 147, loss = 0.22940483\n",
      "Iteration 148, loss = 0.23084451\n",
      "Iteration 149, loss = 0.22904712\n",
      "Iteration 150, loss = 0.22336924\n",
      "Iteration 151, loss = 0.22614069\n",
      "Iteration 152, loss = 0.22308928\n",
      "Iteration 153, loss = 0.22188897\n",
      "Iteration 154, loss = 0.22092018\n",
      "Iteration 155, loss = 0.22003311\n",
      "Iteration 156, loss = 0.21882849\n",
      "Iteration 157, loss = 0.21946827\n",
      "Iteration 158, loss = 0.22163308\n",
      "Iteration 159, loss = 0.21725478\n",
      "Iteration 160, loss = 0.21388235\n",
      "Iteration 161, loss = 0.21194913\n",
      "Iteration 162, loss = 0.21313897\n",
      "Iteration 163, loss = 0.21320682\n",
      "Iteration 164, loss = 0.20806405\n",
      "Iteration 165, loss = 0.20880273\n",
      "Iteration 166, loss = 0.20658995\n",
      "Iteration 167, loss = 0.21029699\n",
      "Iteration 168, loss = 0.20872056\n",
      "Iteration 169, loss = 0.20579431\n",
      "Iteration 170, loss = 0.20385197\n",
      "Iteration 171, loss = 0.20110779\n",
      "Iteration 172, loss = 0.19986072\n",
      "Iteration 173, loss = 0.19948422\n",
      "Iteration 174, loss = 0.19775455\n",
      "Iteration 175, loss = 0.20297245\n",
      "Iteration 176, loss = 0.20065121\n",
      "Iteration 177, loss = 0.19755887\n",
      "Iteration 178, loss = 0.19629074\n",
      "Iteration 179, loss = 0.19640678\n",
      "Iteration 180, loss = 0.19793835\n",
      "Iteration 181, loss = 0.19263070\n",
      "Iteration 182, loss = 0.19153268\n",
      "Iteration 183, loss = 0.19106945\n",
      "Iteration 184, loss = 0.19045312\n",
      "Iteration 185, loss = 0.18888992\n",
      "Iteration 186, loss = 0.18713970\n",
      "Iteration 187, loss = 0.18733831\n",
      "Iteration 188, loss = 0.18663888\n",
      "Iteration 189, loss = 0.18357601\n",
      "Iteration 190, loss = 0.18282309\n",
      "Iteration 191, loss = 0.18241239\n",
      "Iteration 192, loss = 0.18226157\n",
      "Iteration 193, loss = 0.18384344\n",
      "Iteration 194, loss = 0.18295083\n",
      "Iteration 195, loss = 0.17917306\n",
      "Iteration 196, loss = 0.17921365\n",
      "Iteration 197, loss = 0.17924308\n",
      "Iteration 198, loss = 0.18055226\n",
      "Iteration 199, loss = 0.17675135\n",
      "Iteration 200, loss = 0.17540300\n",
      "Iteration 201, loss = 0.17696386\n",
      "Iteration 202, loss = 0.17565829\n",
      "Iteration 203, loss = 0.17779548\n",
      "Iteration 204, loss = 0.17881698\n",
      "Iteration 205, loss = 0.18053622\n",
      "Iteration 206, loss = 0.17448879\n",
      "Iteration 207, loss = 0.16894966\n",
      "Iteration 208, loss = 0.16985264\n",
      "Iteration 209, loss = 0.16969686\n",
      "Iteration 210, loss = 0.16767116\n",
      "Iteration 211, loss = 0.17123707\n",
      "Iteration 212, loss = 0.16814508\n",
      "Iteration 213, loss = 0.16835652\n",
      "Iteration 214, loss = 0.16355098\n",
      "Iteration 215, loss = 0.16402006\n",
      "Iteration 216, loss = 0.16165966\n",
      "Iteration 217, loss = 0.16356228\n",
      "Iteration 218, loss = 0.16323187\n",
      "Iteration 219, loss = 0.16062061\n",
      "Iteration 220, loss = 0.16158395\n",
      "Iteration 221, loss = 0.16101206\n",
      "Iteration 222, loss = 0.15945577\n",
      "Iteration 223, loss = 0.15870084\n",
      "Iteration 224, loss = 0.15861125\n",
      "Iteration 225, loss = 0.15896433\n",
      "Iteration 226, loss = 0.15663143\n",
      "Iteration 227, loss = 0.15449807\n",
      "Iteration 228, loss = 0.15546036\n",
      "Iteration 229, loss = 0.15543776\n",
      "Iteration 230, loss = 0.15221270\n",
      "Iteration 231, loss = 0.15405020\n",
      "Iteration 232, loss = 0.15696424\n",
      "Iteration 233, loss = 0.15130222\n",
      "Iteration 234, loss = 0.14998686\n",
      "Iteration 235, loss = 0.14989153\n",
      "Iteration 236, loss = 0.15184944\n",
      "Iteration 237, loss = 0.14819457\n",
      "Iteration 238, loss = 0.14906376\n",
      "Iteration 239, loss = 0.15061047\n",
      "Iteration 240, loss = 0.15100835\n",
      "Iteration 241, loss = 0.15396090\n",
      "Iteration 242, loss = 0.14664878\n",
      "Iteration 243, loss = 0.14533927\n",
      "Iteration 244, loss = 0.14493104\n",
      "Iteration 245, loss = 0.14327155\n",
      "Iteration 246, loss = 0.14571920\n",
      "Iteration 247, loss = 0.15134695\n",
      "Iteration 248, loss = 0.14330743\n",
      "Iteration 249, loss = 0.14565284\n",
      "Iteration 250, loss = 0.14132428\n",
      "Iteration 251, loss = 0.14220133\n",
      "Iteration 252, loss = 0.14167218\n",
      "Iteration 253, loss = 0.13935027\n",
      "Iteration 254, loss = 0.14094441\n",
      "Iteration 255, loss = 0.14146306\n",
      "Iteration 256, loss = 0.13959007\n",
      "Iteration 257, loss = 0.13864864\n",
      "Iteration 258, loss = 0.13716107\n",
      "Iteration 259, loss = 0.13559866\n",
      "Iteration 260, loss = 0.13918846\n",
      "Iteration 261, loss = 0.13905500\n",
      "Iteration 262, loss = 0.13457414\n",
      "Iteration 263, loss = 0.13878528\n",
      "Iteration 264, loss = 0.13524570\n",
      "Iteration 265, loss = 0.13458585\n",
      "Iteration 266, loss = 0.13944649\n",
      "Iteration 267, loss = 0.13234674\n",
      "Iteration 268, loss = 0.13644318\n",
      "Iteration 269, loss = 0.13445101\n",
      "Iteration 270, loss = 0.13132745\n",
      "Iteration 271, loss = 0.13577116\n",
      "Iteration 272, loss = 0.13004282\n",
      "Iteration 273, loss = 0.12988000\n",
      "Iteration 274, loss = 0.12891598\n",
      "Iteration 275, loss = 0.12793855\n",
      "Iteration 276, loss = 0.13045122\n",
      "Iteration 277, loss = 0.13021482\n",
      "Iteration 278, loss = 0.12840866\n",
      "Iteration 279, loss = 0.12717330\n",
      "Iteration 280, loss = 0.13310317\n",
      "Iteration 281, loss = 0.12558928\n",
      "Iteration 282, loss = 0.12990395\n",
      "Iteration 283, loss = 0.12580411\n",
      "Iteration 284, loss = 0.12610085\n",
      "Iteration 285, loss = 0.12755353\n",
      "Iteration 286, loss = 0.12698984\n",
      "Iteration 287, loss = 0.12676077\n",
      "Iteration 288, loss = 0.12341815\n",
      "Iteration 289, loss = 0.12405679\n",
      "Iteration 290, loss = 0.12589742\n",
      "Iteration 291, loss = 0.12196256\n",
      "Iteration 292, loss = 0.12116611\n",
      "Iteration 293, loss = 0.12598701\n",
      "Iteration 294, loss = 0.12044064\n",
      "Iteration 295, loss = 0.12288204\n",
      "Iteration 296, loss = 0.12561937\n",
      "Iteration 297, loss = 0.12052153\n",
      "Iteration 298, loss = 0.11879775\n",
      "Iteration 299, loss = 0.11930296\n",
      "Iteration 300, loss = 0.11956177\n",
      "Iteration 301, loss = 0.11760609\n",
      "Iteration 302, loss = 0.11989525\n",
      "Iteration 303, loss = 0.11732193\n",
      "Iteration 304, loss = 0.11635328\n",
      "Iteration 305, loss = 0.11782995\n",
      "Iteration 306, loss = 0.11596648\n",
      "Iteration 307, loss = 0.11824653\n",
      "Iteration 308, loss = 0.11643038\n",
      "Iteration 309, loss = 0.11621447\n",
      "Iteration 310, loss = 0.11487804\n",
      "Iteration 311, loss = 0.11424631\n",
      "Iteration 312, loss = 0.11467348\n",
      "Iteration 313, loss = 0.11694472\n",
      "Iteration 314, loss = 0.11382070\n",
      "Iteration 315, loss = 0.11273207\n",
      "Iteration 316, loss = 0.11435005\n",
      "Iteration 317, loss = 0.11269399\n",
      "Iteration 318, loss = 0.11376073\n",
      "Iteration 319, loss = 0.11044942\n",
      "Iteration 320, loss = 0.11408455\n",
      "Iteration 321, loss = 0.11211378\n",
      "Iteration 322, loss = 0.11210413\n",
      "Iteration 323, loss = 0.10998472\n",
      "Iteration 324, loss = 0.11169868\n",
      "Iteration 325, loss = 0.11008163\n",
      "Iteration 326, loss = 0.11057127\n",
      "Iteration 327, loss = 0.11102999\n",
      "Iteration 328, loss = 0.10943561\n",
      "Iteration 329, loss = 0.10930216\n",
      "Iteration 330, loss = 0.10697072\n",
      "Iteration 331, loss = 0.10876108\n",
      "Iteration 332, loss = 0.10717259\n",
      "Iteration 333, loss = 0.10663268\n",
      "Iteration 334, loss = 0.10739335\n",
      "Iteration 335, loss = 0.10682715\n",
      "Iteration 336, loss = 0.10499847\n",
      "Iteration 337, loss = 0.10567508\n",
      "Iteration 338, loss = 0.10789240\n",
      "Iteration 339, loss = 0.10751029\n",
      "Iteration 340, loss = 0.10817280\n",
      "Iteration 341, loss = 0.11369480\n",
      "Iteration 342, loss = 0.10773879\n",
      "Iteration 343, loss = 0.10525030\n",
      "Iteration 344, loss = 0.10480364\n",
      "Iteration 345, loss = 0.10598170\n",
      "Iteration 346, loss = 0.10547957\n",
      "Iteration 347, loss = 0.10781893\n",
      "Iteration 348, loss = 0.10369596\n",
      "Iteration 349, loss = 0.10599710\n",
      "Iteration 350, loss = 0.10793366\n",
      "Iteration 351, loss = 0.10625313\n",
      "Iteration 352, loss = 0.10385123\n",
      "Iteration 353, loss = 0.10050966\n",
      "Iteration 354, loss = 0.10241136\n",
      "Iteration 355, loss = 0.10143892\n",
      "Iteration 356, loss = 0.10554382\n",
      "Iteration 357, loss = 0.10199645\n",
      "Iteration 358, loss = 0.10043844\n",
      "Iteration 359, loss = 0.10123521\n",
      "Iteration 360, loss = 0.09967656\n",
      "Iteration 361, loss = 0.09837093\n",
      "Iteration 362, loss = 0.09963450\n",
      "Iteration 363, loss = 0.09966046\n",
      "Iteration 364, loss = 0.09902662\n",
      "Iteration 365, loss = 0.09775090\n",
      "Iteration 366, loss = 0.09799176\n",
      "Iteration 367, loss = 0.10209552\n",
      "Iteration 368, loss = 0.09836704\n",
      "Iteration 369, loss = 0.09576647\n",
      "Iteration 370, loss = 0.09734219\n",
      "Iteration 371, loss = 0.09648900\n",
      "Iteration 372, loss = 0.09729406\n",
      "Iteration 373, loss = 0.09548982\n",
      "Iteration 374, loss = 0.09543482\n",
      "Iteration 375, loss = 0.09847845\n",
      "Iteration 376, loss = 0.09561615\n",
      "Iteration 377, loss = 0.09669150\n",
      "Iteration 378, loss = 0.09486160\n",
      "Iteration 379, loss = 0.10407367\n",
      "Iteration 380, loss = 0.09553650\n",
      "Iteration 381, loss = 0.09815651\n",
      "Iteration 382, loss = 0.09615269\n",
      "Iteration 383, loss = 0.09735684\n",
      "Iteration 384, loss = 0.09500542\n",
      "Iteration 385, loss = 0.09834225\n",
      "Iteration 386, loss = 0.10067737\n",
      "Iteration 387, loss = 0.10580411\n",
      "Iteration 388, loss = 0.09338871\n",
      "Iteration 389, loss = 0.09236316\n",
      "Iteration 390, loss = 0.09156031\n",
      "Iteration 391, loss = 0.09224301\n",
      "Iteration 392, loss = 0.09917192\n",
      "Iteration 393, loss = 0.09484260\n",
      "Iteration 394, loss = 0.09189267\n",
      "Iteration 395, loss = 0.09373012\n",
      "Iteration 396, loss = 0.09113161\n",
      "Iteration 397, loss = 0.09323869\n",
      "Iteration 398, loss = 0.09664025\n",
      "Iteration 399, loss = 0.09141674\n",
      "Iteration 400, loss = 0.09091995\n",
      "Iteration 401, loss = 0.09101179\n",
      "Iteration 402, loss = 0.09037470\n",
      "Iteration 403, loss = 0.09048297\n",
      "Iteration 404, loss = 0.08910829\n",
      "Iteration 405, loss = 0.08852091\n",
      "Iteration 406, loss = 0.09178514\n",
      "Iteration 407, loss = 0.08815355\n",
      "Iteration 408, loss = 0.08878957\n",
      "Iteration 409, loss = 0.08877886\n",
      "Iteration 410, loss = 0.08705171\n",
      "Iteration 411, loss = 0.09170194\n",
      "Iteration 412, loss = 0.09450200\n",
      "Iteration 413, loss = 0.08709909\n",
      "Iteration 414, loss = 0.08910146\n",
      "Iteration 415, loss = 0.08743528\n",
      "Iteration 416, loss = 0.08817558\n",
      "Iteration 417, loss = 0.08914264\n",
      "Iteration 418, loss = 0.09017260\n",
      "Iteration 419, loss = 0.08756263\n",
      "Iteration 420, loss = 0.08804424\n",
      "Iteration 421, loss = 0.08584916\n",
      "Iteration 422, loss = 0.08850291\n",
      "Iteration 423, loss = 0.09180052\n",
      "Iteration 424, loss = 0.09433540\n",
      "Iteration 425, loss = 0.09669510\n",
      "Iteration 426, loss = 0.09084362\n",
      "Iteration 427, loss = 0.08724501\n",
      "Iteration 428, loss = 0.08537530\n",
      "Iteration 429, loss = 0.08511684\n",
      "Iteration 430, loss = 0.08498351\n",
      "Iteration 431, loss = 0.08478108\n",
      "Iteration 432, loss = 0.08448800\n",
      "Iteration 433, loss = 0.08470625\n",
      "Iteration 434, loss = 0.08483713\n",
      "Iteration 435, loss = 0.08571662\n",
      "Iteration 436, loss = 0.08531054\n",
      "Iteration 437, loss = 0.09361955\n",
      "Iteration 438, loss = 0.08704535\n",
      "Iteration 439, loss = 0.08256109\n",
      "Iteration 440, loss = 0.08579157\n",
      "Iteration 441, loss = 0.08632160\n",
      "Iteration 442, loss = 0.08854041\n",
      "Iteration 443, loss = 0.08609514\n",
      "Iteration 444, loss = 0.08212452\n",
      "Iteration 445, loss = 0.08476694\n",
      "Iteration 446, loss = 0.08484643\n",
      "Iteration 447, loss = 0.08302490\n",
      "Iteration 448, loss = 0.08666712\n",
      "Iteration 449, loss = 0.08180254\n",
      "Iteration 450, loss = 0.08572824\n",
      "Iteration 451, loss = 0.08440120\n",
      "Iteration 452, loss = 0.08426950\n",
      "Iteration 453, loss = 0.08047936\n",
      "Iteration 454, loss = 0.08441631\n",
      "Iteration 455, loss = 0.08681276\n",
      "Iteration 456, loss = 0.08399549\n",
      "Iteration 457, loss = 0.08245238\n",
      "Iteration 458, loss = 0.08134784\n",
      "Iteration 459, loss = 0.08044085\n",
      "Iteration 460, loss = 0.08171621\n",
      "Iteration 461, loss = 0.08201271\n",
      "Iteration 462, loss = 0.07995309\n",
      "Iteration 463, loss = 0.08030417\n",
      "Iteration 464, loss = 0.07881845\n",
      "Iteration 465, loss = 0.08009037\n",
      "Iteration 466, loss = 0.07892214\n",
      "Iteration 467, loss = 0.08621912\n",
      "Iteration 468, loss = 0.08139252\n",
      "Iteration 469, loss = 0.08039414\n",
      "Iteration 470, loss = 0.07894585\n",
      "Iteration 471, loss = 0.08117177\n",
      "Iteration 472, loss = 0.08011738\n",
      "Iteration 473, loss = 0.07977008\n",
      "Iteration 474, loss = 0.07888932\n",
      "Iteration 475, loss = 0.08049245\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "--------------------------------\n",
      "df.shape: (10000, 9)\n",
      "X_train.shape: (8000, 8)\n",
      "X_test.shape: (2000, 8)\n",
      "y_train.shape: (8000,)\n",
      "y_test.shape: (2000,)\n",
      "Training Time: 1.6611 seconds\n",
      "Training Accuracy: 0.9788\n",
      "Validation Accuracy: 0.9845\n",
      "train_error: 0.0212\n",
      "val_error: 0.0155\n"
     ]
    }
   ],
   "source": [
    "# execute the train_MLP function by using df2\n",
    "train_MLP(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69431369\n",
      "Iteration 2, loss = 0.41525636\n",
      "Iteration 3, loss = 0.38845124\n",
      "Iteration 4, loss = 0.37072331\n",
      "Iteration 5, loss = 0.36040574\n",
      "Iteration 6, loss = 0.35288190\n",
      "Iteration 7, loss = 0.34617422\n",
      "Iteration 8, loss = 0.33981327\n",
      "Iteration 9, loss = 0.33355355\n",
      "Iteration 10, loss = 0.32734874\n",
      "Iteration 11, loss = 0.32095607\n",
      "Iteration 12, loss = 0.31476449\n",
      "Iteration 13, loss = 0.30861881\n",
      "Iteration 14, loss = 0.30380889\n",
      "Iteration 15, loss = 0.29691364\n",
      "Iteration 16, loss = 0.29069396\n",
      "Iteration 17, loss = 0.28418421\n",
      "Iteration 18, loss = 0.27796312\n",
      "Iteration 19, loss = 0.27185102\n",
      "Iteration 20, loss = 0.26486811\n",
      "Iteration 21, loss = 0.25902880\n",
      "Iteration 22, loss = 0.25185853\n",
      "Iteration 23, loss = 0.24574043\n",
      "Iteration 24, loss = 0.23851024\n",
      "Iteration 25, loss = 0.23154411\n",
      "Iteration 26, loss = 0.22456809\n",
      "Iteration 27, loss = 0.21780288\n",
      "Iteration 28, loss = 0.21047781\n",
      "Iteration 29, loss = 0.20289327\n",
      "Iteration 30, loss = 0.19593242\n",
      "Iteration 31, loss = 0.18978953\n",
      "Iteration 32, loss = 0.18361558\n",
      "Iteration 33, loss = 0.17591990\n",
      "Iteration 34, loss = 0.16992486\n",
      "Iteration 35, loss = 0.16365990\n",
      "Iteration 36, loss = 0.15819281\n",
      "Iteration 37, loss = 0.15350196\n",
      "Iteration 38, loss = 0.14828643\n",
      "Iteration 39, loss = 0.14473274\n",
      "Iteration 40, loss = 0.13942685\n",
      "Iteration 41, loss = 0.13526984\n",
      "Iteration 42, loss = 0.13235099\n",
      "Iteration 43, loss = 0.12837956\n",
      "Iteration 44, loss = 0.12524842\n",
      "Iteration 45, loss = 0.12125416\n",
      "Iteration 46, loss = 0.11887309\n",
      "Iteration 47, loss = 0.11605543\n",
      "Iteration 48, loss = 0.11366603\n",
      "Iteration 49, loss = 0.11154667\n",
      "Iteration 50, loss = 0.10921757\n",
      "Iteration 51, loss = 0.10706184\n",
      "Iteration 52, loss = 0.10439557\n",
      "Iteration 53, loss = 0.10286585\n",
      "Iteration 54, loss = 0.10086298\n",
      "Iteration 55, loss = 0.09996918\n",
      "Iteration 56, loss = 0.09722428\n",
      "Iteration 57, loss = 0.09545280\n",
      "Iteration 58, loss = 0.09466698\n",
      "Iteration 59, loss = 0.09246738\n",
      "Iteration 60, loss = 0.09141041\n",
      "Iteration 61, loss = 0.08931834\n",
      "Iteration 62, loss = 0.08938060\n",
      "Iteration 63, loss = 0.08712501\n",
      "Iteration 64, loss = 0.08689248\n",
      "Iteration 65, loss = 0.08472440\n",
      "Iteration 66, loss = 0.08304426\n",
      "Iteration 67, loss = 0.08172621\n",
      "Iteration 68, loss = 0.08137131\n",
      "Iteration 69, loss = 0.07937011\n",
      "Iteration 70, loss = 0.07844673\n",
      "Iteration 71, loss = 0.07838808\n",
      "Iteration 72, loss = 0.07754824\n",
      "Iteration 73, loss = 0.07679804\n",
      "Iteration 74, loss = 0.07408546\n",
      "Iteration 75, loss = 0.07416447\n",
      "Iteration 76, loss = 0.07261618\n",
      "Iteration 77, loss = 0.07234301\n",
      "Iteration 78, loss = 0.07195904\n",
      "Iteration 79, loss = 0.07136714\n",
      "Iteration 80, loss = 0.07022379\n",
      "Iteration 81, loss = 0.06833199\n",
      "Iteration 82, loss = 0.06804577\n",
      "Iteration 83, loss = 0.06682750\n",
      "Iteration 84, loss = 0.06728883\n",
      "Iteration 85, loss = 0.06502916\n",
      "Iteration 86, loss = 0.06559810\n",
      "Iteration 87, loss = 0.06435238\n",
      "Iteration 88, loss = 0.06347594\n",
      "Iteration 89, loss = 0.06290516\n",
      "Iteration 90, loss = 0.06262662\n",
      "Iteration 91, loss = 0.06149229\n",
      "Iteration 92, loss = 0.06149389\n",
      "Iteration 93, loss = 0.05979634\n",
      "Iteration 94, loss = 0.05967249\n",
      "Iteration 95, loss = 0.05904985\n",
      "Iteration 96, loss = 0.05848108\n",
      "Iteration 97, loss = 0.05783673\n",
      "Iteration 98, loss = 0.05850650\n",
      "Iteration 99, loss = 0.05738912\n",
      "Iteration 100, loss = 0.05870806\n",
      "Iteration 101, loss = 0.05581737\n",
      "Iteration 102, loss = 0.05534489\n",
      "Iteration 103, loss = 0.05518498\n",
      "Iteration 104, loss = 0.05501377\n",
      "Iteration 105, loss = 0.05370503\n",
      "Iteration 106, loss = 0.05593746\n",
      "Iteration 107, loss = 0.05528963\n",
      "Iteration 108, loss = 0.05461575\n",
      "Iteration 109, loss = 0.05287474\n",
      "Iteration 110, loss = 0.05228424\n",
      "Iteration 111, loss = 0.05220652\n",
      "Iteration 112, loss = 0.05221955\n",
      "Iteration 113, loss = 0.05242123\n",
      "Iteration 114, loss = 0.05190657\n",
      "Iteration 115, loss = 0.05005736\n",
      "Iteration 116, loss = 0.05310590\n",
      "Iteration 117, loss = 0.05144587\n",
      "Iteration 118, loss = 0.05093733\n",
      "Iteration 119, loss = 0.04920816\n",
      "Iteration 120, loss = 0.05085339\n",
      "Iteration 121, loss = 0.04907562\n",
      "Iteration 122, loss = 0.04965804\n",
      "Iteration 123, loss = 0.05041834\n",
      "Iteration 124, loss = 0.05082053\n",
      "Iteration 125, loss = 0.04725001\n",
      "Iteration 126, loss = 0.04857565\n",
      "Iteration 127, loss = 0.04774445\n",
      "Iteration 128, loss = 0.04668679\n",
      "Iteration 129, loss = 0.04806578\n",
      "Iteration 130, loss = 0.04670152\n",
      "Iteration 131, loss = 0.04732705\n",
      "Iteration 132, loss = 0.04717976\n",
      "Iteration 133, loss = 0.04714821\n",
      "Iteration 134, loss = 0.04460116\n",
      "Iteration 135, loss = 0.04834654\n",
      "Iteration 136, loss = 0.04680431\n",
      "Iteration 137, loss = 0.04474505\n",
      "Iteration 138, loss = 0.04539929\n",
      "Iteration 139, loss = 0.04509848\n",
      "Iteration 140, loss = 0.04587829\n",
      "Iteration 141, loss = 0.04492215\n",
      "Iteration 142, loss = 0.04333786\n",
      "Iteration 143, loss = 0.04519378\n",
      "Iteration 144, loss = 0.04214535\n",
      "Iteration 145, loss = 0.04341213\n",
      "Iteration 146, loss = 0.04447273\n",
      "Iteration 147, loss = 0.04422884\n",
      "Iteration 148, loss = 0.04789999\n",
      "Iteration 149, loss = 0.04276199\n",
      "Iteration 150, loss = 0.04253710\n",
      "Iteration 151, loss = 0.04444690\n",
      "Iteration 152, loss = 0.04526908\n",
      "Iteration 153, loss = 0.04132446\n",
      "Iteration 154, loss = 0.04118780\n",
      "Iteration 155, loss = 0.04241771\n",
      "Iteration 156, loss = 0.04109658\n",
      "Iteration 157, loss = 0.04215773\n",
      "Iteration 158, loss = 0.04049074\n",
      "Iteration 159, loss = 0.04111822\n",
      "Iteration 160, loss = 0.04261815\n",
      "Iteration 161, loss = 0.03991082\n",
      "Iteration 162, loss = 0.04003580\n",
      "Iteration 163, loss = 0.04121826\n",
      "Iteration 164, loss = 0.04108730\n",
      "Iteration 165, loss = 0.04055790\n",
      "Iteration 166, loss = 0.04133319\n",
      "Iteration 167, loss = 0.03945186\n",
      "Iteration 168, loss = 0.03932615\n",
      "Iteration 169, loss = 0.04307180\n",
      "Iteration 170, loss = 0.03976598\n",
      "Iteration 171, loss = 0.03916057\n",
      "Iteration 172, loss = 0.04013070\n",
      "Iteration 173, loss = 0.03922914\n",
      "Iteration 174, loss = 0.03777595\n",
      "Iteration 175, loss = 0.03833498\n",
      "Iteration 176, loss = 0.04185037\n",
      "Iteration 177, loss = 0.03951187\n",
      "Iteration 178, loss = 0.03933324\n",
      "Iteration 179, loss = 0.03908429\n",
      "Iteration 180, loss = 0.03736907\n",
      "Iteration 181, loss = 0.03848656\n",
      "Iteration 182, loss = 0.04044454\n",
      "Iteration 183, loss = 0.03750104\n",
      "Iteration 184, loss = 0.03721190\n",
      "Iteration 185, loss = 0.03647707\n",
      "Iteration 186, loss = 0.03939607\n",
      "Iteration 187, loss = 0.03870773\n",
      "Iteration 188, loss = 0.03875829\n",
      "Iteration 189, loss = 0.03639211\n",
      "Iteration 190, loss = 0.03610211\n",
      "Iteration 191, loss = 0.03601949\n",
      "Iteration 192, loss = 0.03613626\n",
      "Iteration 193, loss = 0.03875522\n",
      "Iteration 194, loss = 0.03961011\n",
      "Iteration 195, loss = 0.03716621\n",
      "Iteration 196, loss = 0.03522818\n",
      "Iteration 197, loss = 0.03567990\n",
      "Iteration 198, loss = 0.03563951\n",
      "Iteration 199, loss = 0.03802906\n",
      "Iteration 200, loss = 0.03561514\n",
      "Iteration 201, loss = 0.03655428\n",
      "Iteration 202, loss = 0.03536513\n",
      "Iteration 203, loss = 0.03738589\n",
      "Iteration 204, loss = 0.03666968\n",
      "Iteration 205, loss = 0.03753976\n",
      "Iteration 206, loss = 0.03595053\n",
      "Iteration 207, loss = 0.03751578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "--------------------------------\n",
      "df.shape: (100000, 9)\n",
      "X_train.shape: (80000, 8)\n",
      "X_test.shape: (20000, 8)\n",
      "y_train.shape: (80000,)\n",
      "y_test.shape: (20000,)\n",
      "Training Time: 10.0213 seconds\n",
      "Training Accuracy: 0.9867\n",
      "Validation Accuracy: 0.9871\n",
      "train_error: 0.0133\n",
      "val_error: 0.0129\n"
     ]
    }
   ],
   "source": [
    "# execute the train_MLP function by using df3\n",
    "train_MLP(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_MLP_2hl function : 2 hidden layers, 4 nodes, iteration 500, activation function is relu\n",
    "def  train_MLP_2hl(df):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('outcome', axis=1), df['outcome'], test_size=0.2, random_state=42)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # 2 hidden layers, 4 nodes, iteration 500, activation function is relu\n",
    "    model = MLPClassifier(hidden_layer_sizes=(4,4), activation='relu', max_iter=500, \n",
    "                        verbose=True, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # evaluate the model\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "    val_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "    train_error = 1- train_acc\n",
    "    val_error = 1- val_acc\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "    print(f'df.shape: {df.shape}')\n",
    "    print(f'X_train.shape: {X_train.shape}')\n",
    "    print(f'X_test.shape: {X_test.shape}')\n",
    "    print(f'y_train.shape: {y_train.shape}')\n",
    "    print(f'y_test.shape: {y_test.shape}')\n",
    "    print(f\"Training Time: {total_time:.4f} seconds\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"train_error: {train_error:.4f}\")\n",
    "    print(f\"val_error: {val_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.84133517\n",
      "Iteration 2, loss = 7.74852167\n",
      "Iteration 3, loss = 7.64252664\n",
      "Iteration 4, loss = 7.55430264\n",
      "Iteration 5, loss = 7.44731761\n",
      "Iteration 6, loss = 7.31433956\n",
      "Iteration 7, loss = 7.18590560\n",
      "Iteration 8, loss = 7.03833524\n",
      "Iteration 9, loss = 6.89795491\n",
      "Iteration 10, loss = 6.76396793\n",
      "Iteration 11, loss = 6.61980975\n",
      "Iteration 12, loss = 6.47639250\n",
      "Iteration 13, loss = 6.33505300\n",
      "Iteration 14, loss = 6.19294693\n",
      "Iteration 15, loss = 6.02948946\n",
      "Iteration 16, loss = 5.87321054\n",
      "Iteration 17, loss = 5.69744287\n",
      "Iteration 18, loss = 5.53302325\n",
      "Iteration 19, loss = 5.36340876\n",
      "Iteration 20, loss = 5.17672982\n",
      "Iteration 21, loss = 5.00041039\n",
      "Iteration 22, loss = 4.82984834\n",
      "Iteration 23, loss = 4.68576288\n",
      "Iteration 24, loss = 4.51133672\n",
      "Iteration 25, loss = 4.34846018\n",
      "Iteration 26, loss = 4.19048187\n",
      "Iteration 27, loss = 4.03730667\n",
      "Iteration 28, loss = 3.88916344\n",
      "Iteration 29, loss = 3.73660187\n",
      "Iteration 30, loss = 3.60307985\n",
      "Iteration 31, loss = 3.46512094\n",
      "Iteration 32, loss = 3.33418785\n",
      "Iteration 33, loss = 3.21220674\n",
      "Iteration 34, loss = 3.08544470\n",
      "Iteration 35, loss = 2.96680125\n",
      "Iteration 36, loss = 2.86423194\n",
      "Iteration 37, loss = 2.75228128\n",
      "Iteration 38, loss = 2.65696790\n",
      "Iteration 39, loss = 2.56084578\n",
      "Iteration 40, loss = 2.45567192\n",
      "Iteration 41, loss = 2.36444895\n",
      "Iteration 42, loss = 2.27835724\n",
      "Iteration 43, loss = 2.19863369\n",
      "Iteration 44, loss = 2.11261488\n",
      "Iteration 45, loss = 2.04038240\n",
      "Iteration 46, loss = 1.96339238\n",
      "Iteration 47, loss = 1.89883446\n",
      "Iteration 48, loss = 1.83517228\n",
      "Iteration 49, loss = 1.77268739\n",
      "Iteration 50, loss = 1.72024047\n",
      "Iteration 51, loss = 1.66416449\n",
      "Iteration 52, loss = 1.61291371\n",
      "Iteration 53, loss = 1.57022280\n",
      "Iteration 54, loss = 1.52414798\n",
      "Iteration 55, loss = 1.48447081\n",
      "Iteration 56, loss = 1.44546367\n",
      "Iteration 57, loss = 1.40928406\n",
      "Iteration 58, loss = 1.37627397\n",
      "Iteration 59, loss = 1.34053872\n",
      "Iteration 60, loss = 1.30817949\n",
      "Iteration 61, loss = 1.28185820\n",
      "Iteration 62, loss = 1.25448265\n",
      "Iteration 63, loss = 1.22927442\n",
      "Iteration 64, loss = 1.20394895\n",
      "Iteration 65, loss = 1.18406951\n",
      "Iteration 66, loss = 1.16358807\n",
      "Iteration 67, loss = 1.14068732\n",
      "Iteration 68, loss = 1.12374187\n",
      "Iteration 69, loss = 1.10639042\n",
      "Iteration 70, loss = 1.08867677\n",
      "Iteration 71, loss = 1.07261463\n",
      "Iteration 72, loss = 1.05690749\n",
      "Iteration 73, loss = 1.04043019\n",
      "Iteration 74, loss = 1.02436773\n",
      "Iteration 75, loss = 1.00942079\n",
      "Iteration 76, loss = 0.99622642\n",
      "Iteration 77, loss = 0.98063722\n",
      "Iteration 78, loss = 0.96719725\n",
      "Iteration 79, loss = 0.95498789\n",
      "Iteration 80, loss = 0.94183283\n",
      "Iteration 81, loss = 0.93040506\n",
      "Iteration 82, loss = 0.91706256\n",
      "Iteration 83, loss = 0.90655901\n",
      "Iteration 84, loss = 0.89513689\n",
      "Iteration 85, loss = 0.88503796\n",
      "Iteration 86, loss = 0.87448539\n",
      "Iteration 87, loss = 0.86610240\n",
      "Iteration 88, loss = 0.85692352\n",
      "Iteration 89, loss = 0.84823089\n",
      "Iteration 90, loss = 0.84067902\n",
      "Iteration 91, loss = 0.83285606\n",
      "Iteration 92, loss = 0.82509038\n",
      "Iteration 93, loss = 0.81775669\n",
      "Iteration 94, loss = 0.81091366\n",
      "Iteration 95, loss = 0.80409468\n",
      "Iteration 96, loss = 0.79742557\n",
      "Iteration 97, loss = 0.79126825\n",
      "Iteration 98, loss = 0.78502584\n",
      "Iteration 99, loss = 0.77846416\n",
      "Iteration 100, loss = 0.77243778\n",
      "Iteration 101, loss = 0.76712306\n",
      "Iteration 102, loss = 0.76132016\n",
      "Iteration 103, loss = 0.75628323\n",
      "Iteration 104, loss = 0.75117756\n",
      "Iteration 105, loss = 0.74697131\n",
      "Iteration 106, loss = 0.74215394\n",
      "Iteration 107, loss = 0.73760701\n",
      "Iteration 108, loss = 0.73322531\n",
      "Iteration 109, loss = 0.72874563\n",
      "Iteration 110, loss = 0.72493371\n",
      "Iteration 111, loss = 0.72062511\n",
      "Iteration 112, loss = 0.71627400\n",
      "Iteration 113, loss = 0.71275895\n",
      "Iteration 114, loss = 0.70844656\n",
      "Iteration 115, loss = 0.70424601\n",
      "Iteration 116, loss = 0.69995415\n",
      "Iteration 117, loss = 0.69616137\n",
      "Iteration 118, loss = 0.69237185\n",
      "Iteration 119, loss = 0.68781445\n",
      "Iteration 120, loss = 0.68394907\n",
      "Iteration 121, loss = 0.68020331\n",
      "Iteration 122, loss = 0.67695534\n",
      "Iteration 123, loss = 0.67330310\n",
      "Iteration 124, loss = 0.66994626\n",
      "Iteration 125, loss = 0.66664384\n",
      "Iteration 126, loss = 0.66338415\n",
      "Iteration 127, loss = 0.66065660\n",
      "Iteration 128, loss = 0.65809901\n",
      "Iteration 129, loss = 0.65550216\n",
      "Iteration 130, loss = 0.65257527\n",
      "Iteration 131, loss = 0.65015663\n",
      "Iteration 132, loss = 0.64797443\n",
      "Iteration 133, loss = 0.64595845\n",
      "Iteration 134, loss = 0.64423329\n",
      "Iteration 135, loss = 0.64212066\n",
      "Iteration 136, loss = 0.64041022\n",
      "Iteration 137, loss = 0.63876974\n",
      "Iteration 138, loss = 0.63724489\n",
      "Iteration 139, loss = 0.63546707\n",
      "Iteration 140, loss = 0.63391290\n",
      "Iteration 141, loss = 0.63234278\n",
      "Iteration 142, loss = 0.63098317\n",
      "Iteration 143, loss = 0.62928326\n",
      "Iteration 144, loss = 0.62776837\n",
      "Iteration 145, loss = 0.62617625\n",
      "Iteration 146, loss = 0.62523904\n",
      "Iteration 147, loss = 0.62378073\n",
      "Iteration 148, loss = 0.62248838\n",
      "Iteration 149, loss = 0.62127489\n",
      "Iteration 150, loss = 0.62013096\n",
      "Iteration 151, loss = 0.61893456\n",
      "Iteration 152, loss = 0.61794060\n",
      "Iteration 153, loss = 0.61672561\n",
      "Iteration 154, loss = 0.61569859\n",
      "Iteration 155, loss = 0.61473518\n",
      "Iteration 156, loss = 0.61358471\n",
      "Iteration 157, loss = 0.61283228\n",
      "Iteration 158, loss = 0.61192345\n",
      "Iteration 159, loss = 0.61096986\n",
      "Iteration 160, loss = 0.61008465\n",
      "Iteration 161, loss = 0.60938413\n",
      "Iteration 162, loss = 0.60840411\n",
      "Iteration 163, loss = 0.60762561\n",
      "Iteration 164, loss = 0.60677453\n",
      "Iteration 165, loss = 0.60592943\n",
      "Iteration 166, loss = 0.60522906\n",
      "Iteration 167, loss = 0.60432815\n",
      "Iteration 168, loss = 0.60351375\n",
      "Iteration 169, loss = 0.60255478\n",
      "Iteration 170, loss = 0.60156248\n",
      "Iteration 171, loss = 0.60057566\n",
      "Iteration 172, loss = 0.59974117\n",
      "Iteration 173, loss = 0.59860655\n",
      "Iteration 174, loss = 0.59764712\n",
      "Iteration 175, loss = 0.59686736\n",
      "Iteration 176, loss = 0.59610309\n",
      "Iteration 177, loss = 0.59534750\n",
      "Iteration 178, loss = 0.59454536\n",
      "Iteration 179, loss = 0.59368516\n",
      "Iteration 180, loss = 0.59298226\n",
      "Iteration 181, loss = 0.59230434\n",
      "Iteration 182, loss = 0.59173448\n",
      "Iteration 183, loss = 0.59106560\n",
      "Iteration 184, loss = 0.59043325\n",
      "Iteration 185, loss = 0.58983889\n",
      "Iteration 186, loss = 0.58915595\n",
      "Iteration 187, loss = 0.58863842\n",
      "Iteration 188, loss = 0.58801250\n",
      "Iteration 189, loss = 0.58735628\n",
      "Iteration 190, loss = 0.58679848\n",
      "Iteration 191, loss = 0.58608202\n",
      "Iteration 192, loss = 0.58535190\n",
      "Iteration 193, loss = 0.58474798\n",
      "Iteration 194, loss = 0.58410981\n",
      "Iteration 195, loss = 0.58355247\n",
      "Iteration 196, loss = 0.58291346\n",
      "Iteration 197, loss = 0.58227824\n",
      "Iteration 198, loss = 0.58174727\n",
      "Iteration 199, loss = 0.58097695\n",
      "Iteration 200, loss = 0.58043021\n",
      "Iteration 201, loss = 0.57978366\n",
      "Iteration 202, loss = 0.57922791\n",
      "Iteration 203, loss = 0.57880082\n",
      "Iteration 204, loss = 0.57837331\n",
      "Iteration 205, loss = 0.57780738\n",
      "Iteration 206, loss = 0.57729985\n",
      "Iteration 207, loss = 0.57692163\n",
      "Iteration 208, loss = 0.57654642\n",
      "Iteration 209, loss = 0.57607289\n",
      "Iteration 210, loss = 0.57563313\n",
      "Iteration 211, loss = 0.57530220\n",
      "Iteration 212, loss = 0.57494534\n",
      "Iteration 213, loss = 0.57456570\n",
      "Iteration 214, loss = 0.57419569\n",
      "Iteration 215, loss = 0.57384046\n",
      "Iteration 216, loss = 0.57357622\n",
      "Iteration 217, loss = 0.57331626\n",
      "Iteration 218, loss = 0.57299775\n",
      "Iteration 219, loss = 0.57272173\n",
      "Iteration 220, loss = 0.57242790\n",
      "Iteration 221, loss = 0.57216185\n",
      "Iteration 222, loss = 0.57187161\n",
      "Iteration 223, loss = 0.57160818\n",
      "Iteration 224, loss = 0.57136428\n",
      "Iteration 225, loss = 0.57128564\n",
      "Iteration 226, loss = 0.57106582\n",
      "Iteration 227, loss = 0.57093272\n",
      "Iteration 228, loss = 0.57076279\n",
      "Iteration 229, loss = 0.57054267\n",
      "Iteration 230, loss = 0.57041074\n",
      "Iteration 231, loss = 0.57024268\n",
      "Iteration 232, loss = 0.57001287\n",
      "Iteration 233, loss = 0.56985067\n",
      "Iteration 234, loss = 0.56968087\n",
      "Iteration 235, loss = 0.56951083\n",
      "Iteration 236, loss = 0.56935907\n",
      "Iteration 237, loss = 0.56915354\n",
      "Iteration 238, loss = 0.56903930\n",
      "Iteration 239, loss = 0.56893143\n",
      "Iteration 240, loss = 0.56883534\n",
      "Iteration 241, loss = 0.56879741\n",
      "Iteration 242, loss = 0.56864136\n",
      "Iteration 243, loss = 0.56853021\n",
      "Iteration 244, loss = 0.56843814\n",
      "Iteration 245, loss = 0.56836523\n",
      "Iteration 246, loss = 0.56826035\n",
      "Iteration 247, loss = 0.56817962\n",
      "Iteration 248, loss = 0.56810475\n",
      "Iteration 249, loss = 0.56804499\n",
      "Iteration 250, loss = 0.56796466\n",
      "Iteration 251, loss = 0.56788210\n",
      "Iteration 252, loss = 0.56781297\n",
      "Iteration 253, loss = 0.56770331\n",
      "Iteration 254, loss = 0.56763679\n",
      "Iteration 255, loss = 0.56755258\n",
      "Iteration 256, loss = 0.56747709\n",
      "Iteration 257, loss = 0.56742864\n",
      "Iteration 258, loss = 0.56730551\n",
      "Iteration 259, loss = 0.56723112\n",
      "Iteration 260, loss = 0.56714542\n",
      "Iteration 261, loss = 0.56708770\n",
      "Iteration 262, loss = 0.56703536\n",
      "Iteration 263, loss = 0.56698526\n",
      "Iteration 264, loss = 0.56694701\n",
      "Iteration 265, loss = 0.56693049\n",
      "Iteration 266, loss = 0.56688111\n",
      "Iteration 267, loss = 0.56684038\n",
      "Iteration 268, loss = 0.56678100\n",
      "Iteration 269, loss = 0.56674628\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "--------------------------------\n",
      "df.shape: (1000, 9)\n",
      "X_train.shape: (800, 8)\n",
      "X_test.shape: (200, 8)\n",
      "y_train.shape: (800,)\n",
      "y_test.shape: (200,)\n",
      "Training Time: 0.1597 seconds\n",
      "Training Accuracy: 0.7525\n",
      "Validation Accuracy: 0.6850\n",
      "train_error: 0.2475\n",
      "val_error: 0.3150\n"
     ]
    }
   ],
   "source": [
    "# execute the train_MLP_2hl function by using df1\n",
    "train_MLP_2hl(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.12511110\n",
      "Iteration 2, loss = 5.67830843\n",
      "Iteration 3, loss = 4.08035885\n",
      "Iteration 4, loss = 2.87795808\n",
      "Iteration 5, loss = 2.06889295\n",
      "Iteration 6, loss = 1.56455862\n",
      "Iteration 7, loss = 1.25613707\n",
      "Iteration 8, loss = 1.05493580\n",
      "Iteration 9, loss = 0.92023662\n",
      "Iteration 10, loss = 0.82708790\n",
      "Iteration 11, loss = 0.75838826\n",
      "Iteration 12, loss = 0.71039421\n",
      "Iteration 13, loss = 0.67613235\n",
      "Iteration 14, loss = 0.65252895\n",
      "Iteration 15, loss = 0.63439440\n",
      "Iteration 16, loss = 0.61968489\n",
      "Iteration 17, loss = 0.60625019\n",
      "Iteration 18, loss = 0.59634377\n",
      "Iteration 19, loss = 0.58856745\n",
      "Iteration 20, loss = 0.58277418\n",
      "Iteration 21, loss = 0.57852610\n",
      "Iteration 22, loss = 0.57495985\n",
      "Iteration 23, loss = 0.57201244\n",
      "Iteration 24, loss = 0.56985678\n",
      "Iteration 25, loss = 0.56804915\n",
      "Iteration 26, loss = 0.56625269\n",
      "Iteration 27, loss = 0.56470928\n",
      "Iteration 28, loss = 0.56343288\n",
      "Iteration 29, loss = 0.56245301\n",
      "Iteration 30, loss = 0.56156983\n",
      "Iteration 31, loss = 0.56091245\n",
      "Iteration 32, loss = 0.56032450\n",
      "Iteration 33, loss = 0.55994828\n",
      "Iteration 34, loss = 0.55959562\n",
      "Iteration 35, loss = 0.55924670\n",
      "Iteration 36, loss = 0.55897874\n",
      "Iteration 37, loss = 0.55871916\n",
      "Iteration 38, loss = 0.55840366\n",
      "Iteration 39, loss = 0.55810333\n",
      "Iteration 40, loss = 0.55786550\n",
      "Iteration 41, loss = 0.55770327\n",
      "Iteration 42, loss = 0.55758133\n",
      "Iteration 43, loss = 0.55749287\n",
      "Iteration 44, loss = 0.55742931\n",
      "Iteration 45, loss = 0.55731454\n",
      "Iteration 46, loss = 0.55722732\n",
      "Iteration 47, loss = 0.55708042\n",
      "Iteration 48, loss = 0.55698029\n",
      "Iteration 49, loss = 0.55688528\n",
      "Iteration 50, loss = 0.55687246\n",
      "Iteration 51, loss = 0.55674260\n",
      "Iteration 52, loss = 0.55668734\n",
      "Iteration 53, loss = 0.55663627\n",
      "Iteration 54, loss = 0.55659700\n",
      "Iteration 55, loss = 0.55656091\n",
      "Iteration 56, loss = 0.55653680\n",
      "Iteration 57, loss = 0.55656062\n",
      "Iteration 58, loss = 0.55649655\n",
      "Iteration 59, loss = 0.55641074\n",
      "Iteration 60, loss = 0.55637452\n",
      "Iteration 61, loss = 0.55631304\n",
      "Iteration 62, loss = 0.55624490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "--------------------------------\n",
      "df.shape: (10000, 9)\n",
      "X_train.shape: (8000, 8)\n",
      "X_test.shape: (2000, 8)\n",
      "y_train.shape: (8000,)\n",
      "y_test.shape: (2000,)\n",
      "Training Time: 0.9968 seconds\n",
      "Training Accuracy: 0.7562\n",
      "Validation Accuracy: 0.7580\n",
      "train_error: 0.2438\n",
      "val_error: 0.2420\n"
     ]
    }
   ],
   "source": [
    "# execute the train_MLP_2hl function by using df2\n",
    "train_MLP_2hl(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.69171182\n",
      "Iteration 2, loss = 0.63619136\n",
      "Iteration 3, loss = 0.56553405\n",
      "Iteration 4, loss = 0.55401293\n",
      "Iteration 5, loss = 0.55019499\n",
      "Iteration 6, loss = 0.54842735\n",
      "Iteration 7, loss = 0.44379048\n",
      "Iteration 8, loss = 0.38393482\n",
      "Iteration 9, loss = 0.37491582\n",
      "Iteration 10, loss = 0.36659525\n",
      "Iteration 11, loss = 0.35681921\n",
      "Iteration 12, loss = 0.34056191\n",
      "Iteration 13, loss = 0.31846700\n",
      "Iteration 14, loss = 0.27800727\n",
      "Iteration 15, loss = 0.22391515\n",
      "Iteration 16, loss = 0.17162972\n",
      "Iteration 17, loss = 0.13869292\n",
      "Iteration 18, loss = 0.11871531\n",
      "Iteration 19, loss = 0.10432566\n",
      "Iteration 20, loss = 0.09088550\n",
      "Iteration 21, loss = 0.08315353\n",
      "Iteration 22, loss = 0.07880551\n",
      "Iteration 23, loss = 0.06697418\n",
      "Iteration 24, loss = 0.06564526\n",
      "Iteration 25, loss = 0.06805964\n",
      "Iteration 26, loss = 0.05933942\n",
      "Iteration 27, loss = 0.05782671\n",
      "Iteration 28, loss = 0.05711959\n",
      "Iteration 29, loss = 0.05936395\n",
      "Iteration 30, loss = 0.06069821\n",
      "Iteration 31, loss = 0.04901565\n",
      "Iteration 32, loss = 0.05234702\n",
      "Iteration 33, loss = 0.04899512\n",
      "Iteration 34, loss = 0.04645522\n",
      "Iteration 35, loss = 0.04767903\n",
      "Iteration 36, loss = 0.05368041\n",
      "Iteration 37, loss = 0.04237328\n",
      "Iteration 38, loss = 0.04667780\n",
      "Iteration 39, loss = 0.05430255\n",
      "Iteration 40, loss = 0.04491895\n",
      "Iteration 41, loss = 0.04339744\n",
      "Iteration 42, loss = 0.04194087\n",
      "Iteration 43, loss = 0.05096020\n",
      "Iteration 44, loss = 0.05300448\n",
      "Iteration 45, loss = 0.04243962\n",
      "Iteration 46, loss = 0.04388828\n",
      "Iteration 47, loss = 0.05074694\n",
      "Iteration 48, loss = 0.05550544\n",
      "Iteration 49, loss = 0.04809327\n",
      "Iteration 50, loss = 0.04119675\n",
      "Iteration 51, loss = 0.03832916\n",
      "Iteration 52, loss = 0.05047601\n",
      "Iteration 53, loss = 0.04107100\n",
      "Iteration 54, loss = 0.05407279\n",
      "Iteration 55, loss = 0.04452510\n",
      "Iteration 56, loss = 0.04879251\n",
      "Iteration 57, loss = 0.04053659\n",
      "Iteration 58, loss = 0.04149504\n",
      "Iteration 59, loss = 0.03308883\n",
      "Iteration 60, loss = 0.04161825\n",
      "Iteration 61, loss = 0.04806358\n",
      "Iteration 62, loss = 0.04908919\n",
      "Iteration 63, loss = 0.04124636\n",
      "Iteration 64, loss = 0.04232432\n",
      "Iteration 65, loss = 0.04741746\n",
      "Iteration 66, loss = 0.04632416\n",
      "Iteration 67, loss = 0.04649596\n",
      "Iteration 68, loss = 0.03707507\n",
      "Iteration 69, loss = 0.04687522\n",
      "Iteration 70, loss = 0.04286469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "--------------------------------\n",
      "df.shape: (100000, 9)\n",
      "X_train.shape: (80000, 8)\n",
      "X_test.shape: (20000, 8)\n",
      "y_train.shape: (80000,)\n",
      "y_test.shape: (20000,)\n",
      "Training Time: 5.7436 seconds\n",
      "Training Accuracy: 0.9621\n",
      "Validation Accuracy: 0.9627\n",
      "train_error: 0.0379\n",
      "val_error: 0.0373\n"
     ]
    }
   ],
   "source": [
    "# execute the train_MLP_2hl function by using df3\n",
    "train_MLP_2hl(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Week11 XGBoost\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# train the XGBoost model :\n",
    "# same parameter as above MLP model\n",
    "# train/test split 0.2, random state 42 \n",
    "# n_estimators = 500\n",
    "\n",
    "def XGboost(df):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('outcome', axis=1), df['outcome'], test_size=0.2, random_state=42)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # n_estimators = nrounds\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=500,               # n_estimators is the number of trees in the model\n",
    "        eval_metric=\"logloss\",           # monitor the evaluation metric\n",
    "        use_label_encoder=False,         # close the warning (old version needs)\n",
    "        verbosity=1,                      # close the training process output\n",
    "        early_stopping_rounds=10,        # if 10 rounds are not improved, stop\n",
    "    )\n",
    "\n",
    "    \n",
    "    # provide the validation set to enable early stopping\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],     # provide the validation set\n",
    "        verbose=True,                    # close the training process output\n",
    "     \n",
    "    )\n",
    "\n",
    "    # cross-validation evaluation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    #calculate the time it takes to train the model\n",
    "\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_accs = []\n",
    "    val_accs = []   \n",
    "\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        model = XGBClassifier(n_estimators=500, max_depth=4, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_accs.append(accuracy_score(y_tr, model.predict(X_tr)))\n",
    "        val_accs.append(accuracy_score(y_val, model.predict(X_val)))\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_take = end_time- start_time\n",
    "    print(f'time_take:', time_take)\n",
    "\n",
    "    print(\"Each fold training accuracy:\", train_accs)\n",
    "    print(\"Each fold validation accuracy:\", val_accs)\n",
    "    \n",
    "    trainging_acc = sum(train_accs)/len(train_accs)\n",
    "    val_acc = sum(val_accs)/len(val_accs)\n",
    "\n",
    "    traing_error = 1-trainging_acc\n",
    "    val_error = 1-val_acc\n",
    "\n",
    "    print(f\"training time: {time_take:.4f}\")\n",
    "    print(f\"Average of 5-folds training accuracy: {trainging_acc:.4f}\")\n",
    "    print(f\"Average of 5-folds validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    print(f\"training error:, {traing_error:.4f}\")\n",
    "    print(f\"validation error: , {val_error:4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.44403\n",
      "[1]\tvalidation_0-logloss:0.36386\n",
      "[2]\tvalidation_0-logloss:0.31559\n",
      "[3]\tvalidation_0-logloss:0.27705\n",
      "[4]\tvalidation_0-logloss:0.24733\n",
      "[5]\tvalidation_0-logloss:0.23279\n",
      "[6]\tvalidation_0-logloss:0.21950\n",
      "[7]\tvalidation_0-logloss:0.21514\n",
      "[8]\tvalidation_0-logloss:0.20195\n",
      "[9]\tvalidation_0-logloss:0.19727\n",
      "[10]\tvalidation_0-logloss:0.18991\n",
      "[11]\tvalidation_0-logloss:0.18416\n",
      "[12]\tvalidation_0-logloss:0.18328\n",
      "[13]\tvalidation_0-logloss:0.17657\n",
      "[14]\tvalidation_0-logloss:0.17575\n",
      "[15]\tvalidation_0-logloss:0.17099\n",
      "[16]\tvalidation_0-logloss:0.17119\n",
      "[17]\tvalidation_0-logloss:0.16653\n",
      "[18]\tvalidation_0-logloss:0.16360\n",
      "[19]\tvalidation_0-logloss:0.16280\n",
      "[20]\tvalidation_0-logloss:0.15994\n",
      "[21]\tvalidation_0-logloss:0.15829\n",
      "[22]\tvalidation_0-logloss:0.16128\n",
      "[23]\tvalidation_0-logloss:0.16155\n",
      "[24]\tvalidation_0-logloss:0.16254\n",
      "[25]\tvalidation_0-logloss:0.16098\n",
      "[26]\tvalidation_0-logloss:0.16044\n",
      "[27]\tvalidation_0-logloss:0.15907\n",
      "[28]\tvalidation_0-logloss:0.15768\n",
      "[29]\tvalidation_0-logloss:0.15593\n",
      "[30]\tvalidation_0-logloss:0.15316\n",
      "[31]\tvalidation_0-logloss:0.15352\n",
      "[32]\tvalidation_0-logloss:0.15137\n",
      "[33]\tvalidation_0-logloss:0.15166\n",
      "[34]\tvalidation_0-logloss:0.15102\n",
      "[35]\tvalidation_0-logloss:0.15119\n",
      "[36]\tvalidation_0-logloss:0.15148\n",
      "[37]\tvalidation_0-logloss:0.15268\n",
      "[38]\tvalidation_0-logloss:0.15139\n",
      "[39]\tvalidation_0-logloss:0.14939\n",
      "[40]\tvalidation_0-logloss:0.15212\n",
      "[41]\tvalidation_0-logloss:0.15084\n",
      "[42]\tvalidation_0-logloss:0.14951\n",
      "[43]\tvalidation_0-logloss:0.14761\n",
      "[44]\tvalidation_0-logloss:0.14692\n",
      "[45]\tvalidation_0-logloss:0.14833\n",
      "[46]\tvalidation_0-logloss:0.14680\n",
      "[47]\tvalidation_0-logloss:0.14774\n",
      "[48]\tvalidation_0-logloss:0.14892\n",
      "[49]\tvalidation_0-logloss:0.14678\n",
      "[50]\tvalidation_0-logloss:0.14517\n",
      "[51]\tvalidation_0-logloss:0.14400\n",
      "[52]\tvalidation_0-logloss:0.14567\n",
      "[53]\tvalidation_0-logloss:0.14512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/callback.py:386: UserWarning: [01:33:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54]\tvalidation_0-logloss:0.14352\n",
      "[55]\tvalidation_0-logloss:0.14391\n",
      "[56]\tvalidation_0-logloss:0.14517\n",
      "[57]\tvalidation_0-logloss:0.14437\n",
      "[58]\tvalidation_0-logloss:0.14464\n",
      "[59]\tvalidation_0-logloss:0.14358\n",
      "[60]\tvalidation_0-logloss:0.14319\n",
      "[61]\tvalidation_0-logloss:0.14354\n",
      "[62]\tvalidation_0-logloss:0.14447\n",
      "[63]\tvalidation_0-logloss:0.14454\n",
      "[64]\tvalidation_0-logloss:0.14386\n",
      "[65]\tvalidation_0-logloss:0.14371\n",
      "[66]\tvalidation_0-logloss:0.14219\n",
      "[67]\tvalidation_0-logloss:0.14236\n",
      "[68]\tvalidation_0-logloss:0.14065\n",
      "[69]\tvalidation_0-logloss:0.14103\n",
      "[70]\tvalidation_0-logloss:0.14001\n",
      "[71]\tvalidation_0-logloss:0.13964\n",
      "[72]\tvalidation_0-logloss:0.13935\n",
      "[73]\tvalidation_0-logloss:0.13975\n",
      "[74]\tvalidation_0-logloss:0.14000\n",
      "[75]\tvalidation_0-logloss:0.14026\n",
      "[76]\tvalidation_0-logloss:0.13974\n",
      "[77]\tvalidation_0-logloss:0.14073\n",
      "[78]\tvalidation_0-logloss:0.14118\n",
      "[79]\tvalidation_0-logloss:0.13948\n",
      "[80]\tvalidation_0-logloss:0.13999\n",
      "[81]\tvalidation_0-logloss:0.14055\n",
      "[82]\tvalidation_0-logloss:0.14022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:33:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:33:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:33:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:33:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:33:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_take: 3.6488702297210693\n",
      "Each fold training accuracy: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Each fold validation accuracy: [0.96875, 0.9625, 0.9375, 0.94375, 0.95625]\n",
      "training time: 3.6489\n",
      "Average of 5-folds training accuracy: 1.0000\n",
      "Average of 5-folds validation accuracy: 0.9537\n",
      "training error:, 0.0000\n",
      "validation error: , 0.046250\n"
     ]
    }
   ],
   "source": [
    "# execute the XGboost function by using df1\n",
    "XGboost(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.38566\n",
      "[1]\tvalidation_0-logloss:0.30620\n",
      "[2]\tvalidation_0-logloss:0.25690\n",
      "[3]\tvalidation_0-logloss:0.22215\n",
      "[4]\tvalidation_0-logloss:0.19710\n",
      "[5]\tvalidation_0-logloss:0.17635\n",
      "[6]\tvalidation_0-logloss:0.15913\n",
      "[7]\tvalidation_0-logloss:0.14506\n",
      "[8]\tvalidation_0-logloss:0.13483\n",
      "[9]\tvalidation_0-logloss:0.12642\n",
      "[10]\tvalidation_0-logloss:0.11828\n",
      "[11]\tvalidation_0-logloss:0.11293\n",
      "[12]\tvalidation_0-logloss:0.10787\n",
      "[13]\tvalidation_0-logloss:0.10530\n",
      "[14]\tvalidation_0-logloss:0.10129\n",
      "[15]\tvalidation_0-logloss:0.09817\n",
      "[16]\tvalidation_0-logloss:0.09515\n",
      "[17]\tvalidation_0-logloss:0.09099\n",
      "[18]\tvalidation_0-logloss:0.08873\n",
      "[19]\tvalidation_0-logloss:0.08729\n",
      "[20]\tvalidation_0-logloss:0.08468\n",
      "[21]\tvalidation_0-logloss:0.08363\n",
      "[22]\tvalidation_0-logloss:0.08300\n",
      "[23]\tvalidation_0-logloss:0.08178\n",
      "[24]\tvalidation_0-logloss:0.08034\n",
      "[25]\tvalidation_0-logloss:0.07936\n",
      "[26]\tvalidation_0-logloss:0.07823\n",
      "[27]\tvalidation_0-logloss:0.07750\n",
      "[28]\tvalidation_0-logloss:0.07637\n",
      "[29]\tvalidation_0-logloss:0.07471\n",
      "[30]\tvalidation_0-logloss:0.07408\n",
      "[31]\tvalidation_0-logloss:0.07347\n",
      "[32]\tvalidation_0-logloss:0.07292\n",
      "[33]\tvalidation_0-logloss:0.07318\n",
      "[34]\tvalidation_0-logloss:0.07255\n",
      "[35]\tvalidation_0-logloss:0.07190\n",
      "[36]\tvalidation_0-logloss:0.07133\n",
      "[37]\tvalidation_0-logloss:0.07068\n",
      "[38]\tvalidation_0-logloss:0.07051\n",
      "[39]\tvalidation_0-logloss:0.07100\n",
      "[40]\tvalidation_0-logloss:0.07099\n",
      "[41]\tvalidation_0-logloss:0.07007\n",
      "[42]\tvalidation_0-logloss:0.06920\n",
      "[43]\tvalidation_0-logloss:0.06951\n",
      "[44]\tvalidation_0-logloss:0.06891\n",
      "[45]\tvalidation_0-logloss:0.06868\n",
      "[46]\tvalidation_0-logloss:0.06842\n",
      "[47]\tvalidation_0-logloss:0.06766\n",
      "[48]\tvalidation_0-logloss:0.06770\n",
      "[49]\tvalidation_0-logloss:0.06674\n",
      "[50]\tvalidation_0-logloss:0.06698\n",
      "[51]\tvalidation_0-logloss:0.06639\n",
      "[52]\tvalidation_0-logloss:0.06675\n",
      "[53]\tvalidation_0-logloss:0.06652\n",
      "[54]\tvalidation_0-logloss:0.06634\n",
      "[55]\tvalidation_0-logloss:0.06631\n",
      "[56]\tvalidation_0-logloss:0.06607\n",
      "[57]\tvalidation_0-logloss:0.06582\n",
      "[58]\tvalidation_0-logloss:0.06545\n",
      "[59]\tvalidation_0-logloss:0.06555\n",
      "[60]\tvalidation_0-logloss:0.06514\n",
      "[61]\tvalidation_0-logloss:0.06481\n",
      "[62]\tvalidation_0-logloss:0.06472\n",
      "[63]\tvalidation_0-logloss:0.06480\n",
      "[64]\tvalidation_0-logloss:0.06473\n",
      "[65]\tvalidation_0-logloss:0.06459\n",
      "[66]\tvalidation_0-logloss:0.06474\n",
      "[67]\tvalidation_0-logloss:0.06449\n",
      "[68]\tvalidation_0-logloss:0.06471\n",
      "[69]\tvalidation_0-logloss:0.06493\n",
      "[70]\tvalidation_0-logloss:0.06462\n",
      "[71]\tvalidation_0-logloss:0.06440\n",
      "[72]\tvalidation_0-logloss:0.06378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/callback.py:386: UserWarning: [01:34:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73]\tvalidation_0-logloss:0.06398\n",
      "[74]\tvalidation_0-logloss:0.06416\n",
      "[75]\tvalidation_0-logloss:0.06393\n",
      "[76]\tvalidation_0-logloss:0.06411\n",
      "[77]\tvalidation_0-logloss:0.06364\n",
      "[78]\tvalidation_0-logloss:0.06358\n",
      "[79]\tvalidation_0-logloss:0.06317\n",
      "[80]\tvalidation_0-logloss:0.06272\n",
      "[81]\tvalidation_0-logloss:0.06232\n",
      "[82]\tvalidation_0-logloss:0.06211\n",
      "[83]\tvalidation_0-logloss:0.06223\n",
      "[84]\tvalidation_0-logloss:0.06214\n",
      "[85]\tvalidation_0-logloss:0.06195\n",
      "[86]\tvalidation_0-logloss:0.06168\n",
      "[87]\tvalidation_0-logloss:0.06165\n",
      "[88]\tvalidation_0-logloss:0.06145\n",
      "[89]\tvalidation_0-logloss:0.06125\n",
      "[90]\tvalidation_0-logloss:0.06118\n",
      "[91]\tvalidation_0-logloss:0.06136\n",
      "[92]\tvalidation_0-logloss:0.06147\n",
      "[93]\tvalidation_0-logloss:0.06136\n",
      "[94]\tvalidation_0-logloss:0.06099\n",
      "[95]\tvalidation_0-logloss:0.06109\n",
      "[96]\tvalidation_0-logloss:0.06069\n",
      "[97]\tvalidation_0-logloss:0.06088\n",
      "[98]\tvalidation_0-logloss:0.06098\n",
      "[99]\tvalidation_0-logloss:0.06073\n",
      "[100]\tvalidation_0-logloss:0.06056\n",
      "[101]\tvalidation_0-logloss:0.06038\n",
      "[102]\tvalidation_0-logloss:0.06026\n",
      "[103]\tvalidation_0-logloss:0.06008\n",
      "[104]\tvalidation_0-logloss:0.06026\n",
      "[105]\tvalidation_0-logloss:0.06023\n",
      "[106]\tvalidation_0-logloss:0.06006\n",
      "[107]\tvalidation_0-logloss:0.06012\n",
      "[108]\tvalidation_0-logloss:0.05993\n",
      "[109]\tvalidation_0-logloss:0.06016\n",
      "[110]\tvalidation_0-logloss:0.06021\n",
      "[111]\tvalidation_0-logloss:0.06015\n",
      "[112]\tvalidation_0-logloss:0.06010\n",
      "[113]\tvalidation_0-logloss:0.05999\n",
      "[114]\tvalidation_0-logloss:0.05986\n",
      "[115]\tvalidation_0-logloss:0.05993\n",
      "[116]\tvalidation_0-logloss:0.06017\n",
      "[117]\tvalidation_0-logloss:0.06022\n",
      "[118]\tvalidation_0-logloss:0.05976\n",
      "[119]\tvalidation_0-logloss:0.05969\n",
      "[120]\tvalidation_0-logloss:0.05981\n",
      "[121]\tvalidation_0-logloss:0.05988\n",
      "[122]\tvalidation_0-logloss:0.05970\n",
      "[123]\tvalidation_0-logloss:0.05960\n",
      "[124]\tvalidation_0-logloss:0.05945\n",
      "[125]\tvalidation_0-logloss:0.05946\n",
      "[126]\tvalidation_0-logloss:0.05927\n",
      "[127]\tvalidation_0-logloss:0.05921\n",
      "[128]\tvalidation_0-logloss:0.05903\n",
      "[129]\tvalidation_0-logloss:0.05916\n",
      "[130]\tvalidation_0-logloss:0.05917\n",
      "[131]\tvalidation_0-logloss:0.05941\n",
      "[132]\tvalidation_0-logloss:0.05932\n",
      "[133]\tvalidation_0-logloss:0.05891\n",
      "[134]\tvalidation_0-logloss:0.05857\n",
      "[135]\tvalidation_0-logloss:0.05871\n",
      "[136]\tvalidation_0-logloss:0.05842\n",
      "[137]\tvalidation_0-logloss:0.05854\n",
      "[138]\tvalidation_0-logloss:0.05858\n",
      "[139]\tvalidation_0-logloss:0.05860\n",
      "[140]\tvalidation_0-logloss:0.05873\n",
      "[141]\tvalidation_0-logloss:0.05890\n",
      "[142]\tvalidation_0-logloss:0.05907\n",
      "[143]\tvalidation_0-logloss:0.05907\n",
      "[144]\tvalidation_0-logloss:0.05900\n",
      "[145]\tvalidation_0-logloss:0.05929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_take: 3.20804500579834\n",
      "Each fold training accuracy: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Each fold validation accuracy: [0.97625, 0.97375, 0.983125, 0.96875, 0.9775]\n",
      "training time: 3.2080\n",
      "Average of 5-folds training accuracy: 1.0000\n",
      "Average of 5-folds validation accuracy: 0.9759\n",
      "training error:, 0.0000\n",
      "validation error: , 0.024125\n"
     ]
    }
   ],
   "source": [
    "# execute the XGboost function by using df2\n",
    "XGboost(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.37017\n",
      "[1]\tvalidation_0-logloss:0.28909\n",
      "[2]\tvalidation_0-logloss:0.23778\n",
      "[3]\tvalidation_0-logloss:0.20007\n",
      "[4]\tvalidation_0-logloss:0.17367\n",
      "[5]\tvalidation_0-logloss:0.15350\n",
      "[6]\tvalidation_0-logloss:0.13776\n",
      "[7]\tvalidation_0-logloss:0.12565\n",
      "[8]\tvalidation_0-logloss:0.11603\n",
      "[9]\tvalidation_0-logloss:0.10771\n",
      "[10]\tvalidation_0-logloss:0.10065\n",
      "[11]\tvalidation_0-logloss:0.09572\n",
      "[12]\tvalidation_0-logloss:0.09022\n",
      "[13]\tvalidation_0-logloss:0.08628\n",
      "[14]\tvalidation_0-logloss:0.08337\n",
      "[15]\tvalidation_0-logloss:0.07997\n",
      "[16]\tvalidation_0-logloss:0.07751\n",
      "[17]\tvalidation_0-logloss:0.07543\n",
      "[18]\tvalidation_0-logloss:0.07292\n",
      "[19]\tvalidation_0-logloss:0.07046\n",
      "[20]\tvalidation_0-logloss:0.06874\n",
      "[21]\tvalidation_0-logloss:0.06735\n",
      "[22]\tvalidation_0-logloss:0.06552\n",
      "[23]\tvalidation_0-logloss:0.06386\n",
      "[24]\tvalidation_0-logloss:0.06258\n",
      "[25]\tvalidation_0-logloss:0.06138\n",
      "[26]\tvalidation_0-logloss:0.05998\n",
      "[27]\tvalidation_0-logloss:0.05878\n",
      "[28]\tvalidation_0-logloss:0.05780\n",
      "[29]\tvalidation_0-logloss:0.05670\n",
      "[30]\tvalidation_0-logloss:0.05581\n",
      "[31]\tvalidation_0-logloss:0.05510\n",
      "[32]\tvalidation_0-logloss:0.05425\n",
      "[33]\tvalidation_0-logloss:0.05340\n",
      "[34]\tvalidation_0-logloss:0.05274\n",
      "[35]\tvalidation_0-logloss:0.05203\n",
      "[36]\tvalidation_0-logloss:0.05129\n",
      "[37]\tvalidation_0-logloss:0.05085\n",
      "[38]\tvalidation_0-logloss:0.05015\n",
      "[39]\tvalidation_0-logloss:0.04978\n",
      "[40]\tvalidation_0-logloss:0.04896\n",
      "[41]\tvalidation_0-logloss:0.04835\n",
      "[42]\tvalidation_0-logloss:0.04778\n",
      "[43]\tvalidation_0-logloss:0.04728\n",
      "[44]\tvalidation_0-logloss:0.04667\n",
      "[45]\tvalidation_0-logloss:0.04634\n",
      "[46]\tvalidation_0-logloss:0.04605\n",
      "[47]\tvalidation_0-logloss:0.04557\n",
      "[48]\tvalidation_0-logloss:0.04506\n",
      "[49]\tvalidation_0-logloss:0.04471\n",
      "[50]\tvalidation_0-logloss:0.04446\n",
      "[51]\tvalidation_0-logloss:0.04409\n",
      "[52]\tvalidation_0-logloss:0.04363\n",
      "[53]\tvalidation_0-logloss:0.04327\n",
      "[54]\tvalidation_0-logloss:0.04296\n",
      "[55]\tvalidation_0-logloss:0.04255\n",
      "[56]\tvalidation_0-logloss:0.04229\n",
      "[57]\tvalidation_0-logloss:0.04179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/callback.py:386: UserWarning: [01:34:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58]\tvalidation_0-logloss:0.04148\n",
      "[59]\tvalidation_0-logloss:0.04117\n",
      "[60]\tvalidation_0-logloss:0.04093\n",
      "[61]\tvalidation_0-logloss:0.04075\n",
      "[62]\tvalidation_0-logloss:0.04048\n",
      "[63]\tvalidation_0-logloss:0.04023\n",
      "[64]\tvalidation_0-logloss:0.04001\n",
      "[65]\tvalidation_0-logloss:0.03980\n",
      "[66]\tvalidation_0-logloss:0.03957\n",
      "[67]\tvalidation_0-logloss:0.03941\n",
      "[68]\tvalidation_0-logloss:0.03904\n",
      "[69]\tvalidation_0-logloss:0.03877\n",
      "[70]\tvalidation_0-logloss:0.03853\n",
      "[71]\tvalidation_0-logloss:0.03838\n",
      "[72]\tvalidation_0-logloss:0.03825\n",
      "[73]\tvalidation_0-logloss:0.03809\n",
      "[74]\tvalidation_0-logloss:0.03780\n",
      "[75]\tvalidation_0-logloss:0.03772\n",
      "[76]\tvalidation_0-logloss:0.03761\n",
      "[77]\tvalidation_0-logloss:0.03744\n",
      "[78]\tvalidation_0-logloss:0.03729\n",
      "[79]\tvalidation_0-logloss:0.03708\n",
      "[80]\tvalidation_0-logloss:0.03687\n",
      "[81]\tvalidation_0-logloss:0.03665\n",
      "[82]\tvalidation_0-logloss:0.03653\n",
      "[83]\tvalidation_0-logloss:0.03631\n",
      "[84]\tvalidation_0-logloss:0.03615\n",
      "[85]\tvalidation_0-logloss:0.03596\n",
      "[86]\tvalidation_0-logloss:0.03583\n",
      "[87]\tvalidation_0-logloss:0.03558\n",
      "[88]\tvalidation_0-logloss:0.03544\n",
      "[89]\tvalidation_0-logloss:0.03529\n",
      "[90]\tvalidation_0-logloss:0.03516\n",
      "[91]\tvalidation_0-logloss:0.03505\n",
      "[92]\tvalidation_0-logloss:0.03493\n",
      "[93]\tvalidation_0-logloss:0.03486\n",
      "[94]\tvalidation_0-logloss:0.03479\n",
      "[95]\tvalidation_0-logloss:0.03462\n",
      "[96]\tvalidation_0-logloss:0.03450\n",
      "[97]\tvalidation_0-logloss:0.03441\n",
      "[98]\tvalidation_0-logloss:0.03423\n",
      "[99]\tvalidation_0-logloss:0.03420\n",
      "[100]\tvalidation_0-logloss:0.03417\n",
      "[101]\tvalidation_0-logloss:0.03405\n",
      "[102]\tvalidation_0-logloss:0.03385\n",
      "[103]\tvalidation_0-logloss:0.03377\n",
      "[104]\tvalidation_0-logloss:0.03369\n",
      "[105]\tvalidation_0-logloss:0.03359\n",
      "[106]\tvalidation_0-logloss:0.03353\n",
      "[107]\tvalidation_0-logloss:0.03333\n",
      "[108]\tvalidation_0-logloss:0.03325\n",
      "[109]\tvalidation_0-logloss:0.03304\n",
      "[110]\tvalidation_0-logloss:0.03295\n",
      "[111]\tvalidation_0-logloss:0.03282\n",
      "[112]\tvalidation_0-logloss:0.03272\n",
      "[113]\tvalidation_0-logloss:0.03259\n",
      "[114]\tvalidation_0-logloss:0.03249\n",
      "[115]\tvalidation_0-logloss:0.03237\n",
      "[116]\tvalidation_0-logloss:0.03233\n",
      "[117]\tvalidation_0-logloss:0.03228\n",
      "[118]\tvalidation_0-logloss:0.03212\n",
      "[119]\tvalidation_0-logloss:0.03209\n",
      "[120]\tvalidation_0-logloss:0.03198\n",
      "[121]\tvalidation_0-logloss:0.03185\n",
      "[122]\tvalidation_0-logloss:0.03184\n",
      "[123]\tvalidation_0-logloss:0.03174\n",
      "[124]\tvalidation_0-logloss:0.03176\n",
      "[125]\tvalidation_0-logloss:0.03171\n",
      "[126]\tvalidation_0-logloss:0.03163\n",
      "[127]\tvalidation_0-logloss:0.03160\n",
      "[128]\tvalidation_0-logloss:0.03155\n",
      "[129]\tvalidation_0-logloss:0.03142\n",
      "[130]\tvalidation_0-logloss:0.03135\n",
      "[131]\tvalidation_0-logloss:0.03130\n",
      "[132]\tvalidation_0-logloss:0.03113\n",
      "[133]\tvalidation_0-logloss:0.03111\n",
      "[134]\tvalidation_0-logloss:0.03115\n",
      "[135]\tvalidation_0-logloss:0.03107\n",
      "[136]\tvalidation_0-logloss:0.03099\n",
      "[137]\tvalidation_0-logloss:0.03099\n",
      "[138]\tvalidation_0-logloss:0.03097\n",
      "[139]\tvalidation_0-logloss:0.03092\n",
      "[140]\tvalidation_0-logloss:0.03082\n",
      "[141]\tvalidation_0-logloss:0.03074\n",
      "[142]\tvalidation_0-logloss:0.03066\n",
      "[143]\tvalidation_0-logloss:0.03062\n",
      "[144]\tvalidation_0-logloss:0.03044\n",
      "[145]\tvalidation_0-logloss:0.03039\n",
      "[146]\tvalidation_0-logloss:0.03031\n",
      "[147]\tvalidation_0-logloss:0.03025\n",
      "[148]\tvalidation_0-logloss:0.03013\n",
      "[149]\tvalidation_0-logloss:0.03006\n",
      "[150]\tvalidation_0-logloss:0.03009\n",
      "[151]\tvalidation_0-logloss:0.03005\n",
      "[152]\tvalidation_0-logloss:0.03003\n",
      "[153]\tvalidation_0-logloss:0.03003\n",
      "[154]\tvalidation_0-logloss:0.02990\n",
      "[155]\tvalidation_0-logloss:0.02983\n",
      "[156]\tvalidation_0-logloss:0.02976\n",
      "[157]\tvalidation_0-logloss:0.02970\n",
      "[158]\tvalidation_0-logloss:0.02967\n",
      "[159]\tvalidation_0-logloss:0.02971\n",
      "[160]\tvalidation_0-logloss:0.02966\n",
      "[161]\tvalidation_0-logloss:0.02967\n",
      "[162]\tvalidation_0-logloss:0.02962\n",
      "[163]\tvalidation_0-logloss:0.02954\n",
      "[164]\tvalidation_0-logloss:0.02944\n",
      "[165]\tvalidation_0-logloss:0.02941\n",
      "[166]\tvalidation_0-logloss:0.02930\n",
      "[167]\tvalidation_0-logloss:0.02924\n",
      "[168]\tvalidation_0-logloss:0.02920\n",
      "[169]\tvalidation_0-logloss:0.02925\n",
      "[170]\tvalidation_0-logloss:0.02915\n",
      "[171]\tvalidation_0-logloss:0.02903\n",
      "[172]\tvalidation_0-logloss:0.02897\n",
      "[173]\tvalidation_0-logloss:0.02893\n",
      "[174]\tvalidation_0-logloss:0.02888\n",
      "[175]\tvalidation_0-logloss:0.02878\n",
      "[176]\tvalidation_0-logloss:0.02878\n",
      "[177]\tvalidation_0-logloss:0.02874\n",
      "[178]\tvalidation_0-logloss:0.02878\n",
      "[179]\tvalidation_0-logloss:0.02873\n",
      "[180]\tvalidation_0-logloss:0.02865\n",
      "[181]\tvalidation_0-logloss:0.02862\n",
      "[182]\tvalidation_0-logloss:0.02851\n",
      "[183]\tvalidation_0-logloss:0.02850\n",
      "[184]\tvalidation_0-logloss:0.02845\n",
      "[185]\tvalidation_0-logloss:0.02842\n",
      "[186]\tvalidation_0-logloss:0.02838\n",
      "[187]\tvalidation_0-logloss:0.02828\n",
      "[188]\tvalidation_0-logloss:0.02828\n",
      "[189]\tvalidation_0-logloss:0.02827\n",
      "[190]\tvalidation_0-logloss:0.02825\n",
      "[191]\tvalidation_0-logloss:0.02816\n",
      "[192]\tvalidation_0-logloss:0.02815\n",
      "[193]\tvalidation_0-logloss:0.02811\n",
      "[194]\tvalidation_0-logloss:0.02808\n",
      "[195]\tvalidation_0-logloss:0.02808\n",
      "[196]\tvalidation_0-logloss:0.02804\n",
      "[197]\tvalidation_0-logloss:0.02805\n",
      "[198]\tvalidation_0-logloss:0.02808\n",
      "[199]\tvalidation_0-logloss:0.02801\n",
      "[200]\tvalidation_0-logloss:0.02797\n",
      "[201]\tvalidation_0-logloss:0.02798\n",
      "[202]\tvalidation_0-logloss:0.02794\n",
      "[203]\tvalidation_0-logloss:0.02796\n",
      "[204]\tvalidation_0-logloss:0.02786\n",
      "[205]\tvalidation_0-logloss:0.02785\n",
      "[206]\tvalidation_0-logloss:0.02783\n",
      "[207]\tvalidation_0-logloss:0.02769\n",
      "[208]\tvalidation_0-logloss:0.02769\n",
      "[209]\tvalidation_0-logloss:0.02770\n",
      "[210]\tvalidation_0-logloss:0.02765\n",
      "[211]\tvalidation_0-logloss:0.02765\n",
      "[212]\tvalidation_0-logloss:0.02761\n",
      "[213]\tvalidation_0-logloss:0.02762\n",
      "[214]\tvalidation_0-logloss:0.02761\n",
      "[215]\tvalidation_0-logloss:0.02756\n",
      "[216]\tvalidation_0-logloss:0.02756\n",
      "[217]\tvalidation_0-logloss:0.02754\n",
      "[218]\tvalidation_0-logloss:0.02747\n",
      "[219]\tvalidation_0-logloss:0.02750\n",
      "[220]\tvalidation_0-logloss:0.02748\n",
      "[221]\tvalidation_0-logloss:0.02750\n",
      "[222]\tvalidation_0-logloss:0.02744\n",
      "[223]\tvalidation_0-logloss:0.02745\n",
      "[224]\tvalidation_0-logloss:0.02744\n",
      "[225]\tvalidation_0-logloss:0.02735\n",
      "[226]\tvalidation_0-logloss:0.02733\n",
      "[227]\tvalidation_0-logloss:0.02723\n",
      "[228]\tvalidation_0-logloss:0.02722\n",
      "[229]\tvalidation_0-logloss:0.02724\n",
      "[230]\tvalidation_0-logloss:0.02724\n",
      "[231]\tvalidation_0-logloss:0.02723\n",
      "[232]\tvalidation_0-logloss:0.02724\n",
      "[233]\tvalidation_0-logloss:0.02718\n",
      "[234]\tvalidation_0-logloss:0.02716\n",
      "[235]\tvalidation_0-logloss:0.02716\n",
      "[236]\tvalidation_0-logloss:0.02713\n",
      "[237]\tvalidation_0-logloss:0.02715\n",
      "[238]\tvalidation_0-logloss:0.02706\n",
      "[239]\tvalidation_0-logloss:0.02708\n",
      "[240]\tvalidation_0-logloss:0.02706\n",
      "[241]\tvalidation_0-logloss:0.02703\n",
      "[242]\tvalidation_0-logloss:0.02700\n",
      "[243]\tvalidation_0-logloss:0.02693\n",
      "[244]\tvalidation_0-logloss:0.02692\n",
      "[245]\tvalidation_0-logloss:0.02686\n",
      "[246]\tvalidation_0-logloss:0.02690\n",
      "[247]\tvalidation_0-logloss:0.02686\n",
      "[248]\tvalidation_0-logloss:0.02688\n",
      "[249]\tvalidation_0-logloss:0.02681\n",
      "[250]\tvalidation_0-logloss:0.02681\n",
      "[251]\tvalidation_0-logloss:0.02683\n",
      "[252]\tvalidation_0-logloss:0.02682\n",
      "[253]\tvalidation_0-logloss:0.02680\n",
      "[254]\tvalidation_0-logloss:0.02673\n",
      "[255]\tvalidation_0-logloss:0.02672\n",
      "[256]\tvalidation_0-logloss:0.02669\n",
      "[257]\tvalidation_0-logloss:0.02670\n",
      "[258]\tvalidation_0-logloss:0.02668\n",
      "[259]\tvalidation_0-logloss:0.02658\n",
      "[260]\tvalidation_0-logloss:0.02664\n",
      "[261]\tvalidation_0-logloss:0.02670\n",
      "[262]\tvalidation_0-logloss:0.02672\n",
      "[263]\tvalidation_0-logloss:0.02675\n",
      "[264]\tvalidation_0-logloss:0.02669\n",
      "[265]\tvalidation_0-logloss:0.02667\n",
      "[266]\tvalidation_0-logloss:0.02668\n",
      "[267]\tvalidation_0-logloss:0.02662\n",
      "[268]\tvalidation_0-logloss:0.02667\n",
      "[269]\tvalidation_0-logloss:0.02662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/Users/sandyliu/anaconda3/envs/Class_HDS5330/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [01:34:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_take: 4.626389026641846\n",
      "Each fold training accuracy: [0.999234375, 0.99928125, 0.999234375, 0.99940625, 0.99921875]\n",
      "Each fold validation accuracy: [0.9890625, 0.988625, 0.98825, 0.98875, 0.98875]\n",
      "training time: 4.6264\n",
      "Average of 5-folds training accuracy: 0.9993\n",
      "Average of 5-folds validation accuracy: 0.9887\n",
      "training error:, 0.0007\n",
      "validation error: , 0.011313\n"
     ]
    }
   ],
   "source": [
    "# execute the XGboost function by using df3\n",
    "XGboost(df3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Class_HDS5330)",
   "language": "python",
   "name": "class_hds5330"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
